This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
data-common/
  src/
    main/
      java/
        com/
          platform/
            data/
              common/
                config/
                  TenantConfig.java
                mapper/
                  UdtMapper.java
                model/
                  PartitionKey.java
                registry/
                  TenantConfigRegistry.java
                util/
                  PartitionCalculator.java
    test/
      java/
        com/
          platform/
            data/
              common/
                mapper/
                  UdtMapperTest.java
                model/
                  PartitionKeyTest.java
                registry/
                  TenantConfigRegistryTest.java
                util/
                  PartitionCalculatorTest.java
  pom.xml
data-ingest-service/
  src/
    main/
      java/
        com/
          platform/
            data/
              ingest/
                config/
                  CassandraConfig.java
                  KafkaConfig.java
                controller/
                  data-platform-core.code-workspace
                  IngestController.java
                dto/
                  IngestBatchRequest.java
                kafka/
                  KafkaIngestConsumer.java
                service/
                  DynamicIngestService.java
                IngestServiceApplication.java
      resources/
        application-kafka.properties
        application.properties
    test/
      java/
        com/
          platform/
            data/
              ingest/
                integration/
                  IngestServiceIntegrationTest.java
  pom.xml
data-platform-agent/
  app/
    __init__.py
    agent.py
    main.py
    schemas.py
    tools.py
  requirements.txt
data-platform-mcp-server/
  .mvn/
    wrapper/
      maven-wrapper.properties
  src/
    main/
      java/
        com/
          example/
            dataplatformmcpserver/
              DataPlatformMcpServerApplication.java
      resources/
        application.properties
    test/
      java/
        com/
          example/
            dataplatformmcpserver/
              DataPlatformMcpServerApplicationTests.java
  .gitattributes
  .gitignore
  mvnw
  mvnw.cmd
  pom.xml
data-query-service/
  src/
    main/
      java/
        com/
          platform/
            data/
              query/
                config/
                  CassandraConfig.java
                controller/
                  QueryController.java
                service/
                  DynamicRetrievalService.java
                QueryServiceApplication.java
      resources/
        application.properties
    test/
      java/
        com/
          platform/
            data/
              query/
                GenericPlatformIntegrationTest.java
  pom.xml
.gitignore
pom.xml
PROJECT_SUMMARY.md
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="data-platform-agent/app/__init__.py">

</file>

<file path="data-platform-agent/app/agent.py">
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate
from app.tools import fetch_daily_data

# Initialize LLM
llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro", temperature=0)

# Define Tools
tools = [fetch_daily_data]

# Define System Prompt
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a financial data assistant. You have access to a specific tool 'fetch_daily_data'. "
               "Always use this tool when asked about prices or market data. "
               "The user provides the tenant_id in the context: {tenant_id}. "
               "You MUST pass this tenant_id to the tool when calling it. "
               "You must extract the dates and ticker from the question."),
    ("human", "{input}"),
    ("placeholder", "{agent_scratchpad}"),
])

# Create Agent
agent = create_tool_calling_agent(llm, tools, prompt)

# Create Executor
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
</file>

<file path="data-platform-agent/app/main.py">
from fastapi import FastAPI, HTTPException
from app.schemas import ChatRequest, ChatResponse
from app.agent import agent_executor

app = FastAPI(title="Data Platform Agent")

@app.post("/api/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    try:
        # Pass tenant_id and input (query) to the agent
        # The prompt template expects 'input' and we want to make 'tenant_id' available for the agent to use
        result = await agent_executor.ainvoke({
            "input": request.query,
            "tenant_id": request.tenant_id
        })
        
        # Extract the answer
        answer = result.get("output", "")
        
        # Attempt to determine tool usage from intermediate steps if available, 
        # but standard AgentExecutor result 'output' is the final string.
        # Capturing tool usage requires parsing intermediate_steps if return_intermediate_steps=True
        # For this simple requirement, we'll leave tool_used as None or implement logic if needed.
        # To keep it simple as per requirements, we just return the answer.
        
        return ChatResponse(answer=answer, tool_used="fetch_daily_data" if "fetch_daily_data" in str(result) else None)

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
</file>

<file path="data-platform-agent/app/schemas.py">
from pydantic import BaseModel
from typing import Optional

class ChatRequest(BaseModel):
    query: str
    tenant_id: str

class ChatResponse(BaseModel):
    answer: str
    tool_used: Optional[str] = None
</file>

<file path="data-platform-agent/app/tools.py">
import requests
from langchain.tools import tool
import json

@tool
def fetch_daily_data(ticker: str, start_date: str, end_date: str, tenant_id: str = "DEFAULT") -> str:
    """
    Fetches daily pricing data for a given ticker and date range.
    
    Args:
        ticker (str): The instrument ID (e.g., IBM).
        start_date (str): The start date in 'YYYY-MM-DD' format.
        end_date (str): The end date in 'YYYY-MM-DD' format.
        tenant_id (str): The tenant ID context for the query. Defaults to "DEFAULT" if not provided, 
                        but usually should be passed from the agent's context.

    Returns:
        str: JSON string response from the Java Query Service.
    """
    print(f"DEBUG: fetch_daily_data called with ticker={ticker}, start={start_date}, end={end_date}, tenant={tenant_id}")
    
    url = f"http://localhost:8082/api/query/{tenant_id}/DAILY"
    payload = {
        "instrument_id": ticker,
        "start_date": start_date,
        "end_date": end_date
    }
    
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return json.dumps(response.json())
    except requests.exceptions.RequestException as e:
        return f"Error fetching daily data: {str(e)}"
</file>

<file path="data-platform-agent/requirements.txt">
fastapi
uvicorn[standard]
pydantic>=2.0
langchain
langchain-google-genai
requests
</file>

<file path="data-platform-mcp-server/.mvn/wrapper/maven-wrapper.properties">
wrapperVersion=3.3.4
distributionType=only-script
distributionUrl=https://repo.maven.apache.org/maven2/org/apache/maven/apache-maven/3.9.12/apache-maven-3.9.12-bin.zip
</file>

<file path="data-platform-mcp-server/src/main/java/com/example/dataplatformmcpserver/DataPlatformMcpServerApplication.java">
package com.example.dataplatformmcpserver;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class DataPlatformMcpServerApplication {

    public static void main(String[] args) {
        SpringApplication.run(DataPlatformMcpServerApplication.class, args);
    }

}
</file>

<file path="data-platform-mcp-server/src/main/resources/application.properties">
spring.application.name=data-platform-mcp-server
</file>

<file path="data-platform-mcp-server/src/test/java/com/example/dataplatformmcpserver/DataPlatformMcpServerApplicationTests.java">
package com.example.dataplatformmcpserver;

import org.junit.jupiter.api.Test;
import org.springframework.boot.test.context.SpringBootTest;

@SpringBootTest
class DataPlatformMcpServerApplicationTests {

    @Test
    void contextLoads() {
    }

}
</file>

<file path="data-platform-mcp-server/.gitattributes">
/mvnw text eol=lf
*.cmd text eol=crlf
</file>

<file path="data-platform-mcp-server/.gitignore">
HELP.md
target/
.mvn/wrapper/maven-wrapper.jar
!**/src/main/**/target/
!**/src/test/**/target/

### STS ###
.apt_generated
.classpath
.factorypath
.project
.settings
.springBeans
.sts4-cache

### IntelliJ IDEA ###
.idea
*.iws
*.iml
*.ipr

### NetBeans ###
/nbproject/private/
/nbbuild/
/dist/
/nbdist/
/.nb-gradle/
build/
!**/src/main/**/build/
!**/src/test/**/build/

### VS Code ###
.vscode/
</file>

<file path="data-platform-mcp-server/mvnw">
#!/bin/sh
# ----------------------------------------------------------------------------
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
# ----------------------------------------------------------------------------

# ----------------------------------------------------------------------------
# Apache Maven Wrapper startup batch script, version 3.3.4
#
# Optional ENV vars
# -----------------
#   JAVA_HOME - location of a JDK home dir, required when download maven via java source
#   MVNW_REPOURL - repo url base for downloading maven distribution
#   MVNW_USERNAME/MVNW_PASSWORD - user and password for downloading maven
#   MVNW_VERBOSE - true: enable verbose log; debug: trace the mvnw script; others: silence the output
# ----------------------------------------------------------------------------

set -euf
[ "${MVNW_VERBOSE-}" != debug ] || set -x

# OS specific support.
native_path() { printf %s\\n "$1"; }
case "$(uname)" in
CYGWIN* | MINGW*)
  [ -z "${JAVA_HOME-}" ] || JAVA_HOME="$(cygpath --unix "$JAVA_HOME")"
  native_path() { cygpath --path --windows "$1"; }
  ;;
esac

# set JAVACMD and JAVACCMD
set_java_home() {
  # For Cygwin and MinGW, ensure paths are in Unix format before anything is touched
  if [ -n "${JAVA_HOME-}" ]; then
    if [ -x "$JAVA_HOME/jre/sh/java" ]; then
      # IBM's JDK on AIX uses strange locations for the executables
      JAVACMD="$JAVA_HOME/jre/sh/java"
      JAVACCMD="$JAVA_HOME/jre/sh/javac"
    else
      JAVACMD="$JAVA_HOME/bin/java"
      JAVACCMD="$JAVA_HOME/bin/javac"

      if [ ! -x "$JAVACMD" ] || [ ! -x "$JAVACCMD" ]; then
        echo "The JAVA_HOME environment variable is not defined correctly, so mvnw cannot run." >&2
        echo "JAVA_HOME is set to \"$JAVA_HOME\", but \"\$JAVA_HOME/bin/java\" or \"\$JAVA_HOME/bin/javac\" does not exist." >&2
        return 1
      fi
    fi
  else
    JAVACMD="$(
      'set' +e
      'unset' -f command 2>/dev/null
      'command' -v java
    )" || :
    JAVACCMD="$(
      'set' +e
      'unset' -f command 2>/dev/null
      'command' -v javac
    )" || :

    if [ ! -x "${JAVACMD-}" ] || [ ! -x "${JAVACCMD-}" ]; then
      echo "The java/javac command does not exist in PATH nor is JAVA_HOME set, so mvnw cannot run." >&2
      return 1
    fi
  fi
}

# hash string like Java String::hashCode
hash_string() {
  str="${1:-}" h=0
  while [ -n "$str" ]; do
    char="${str%"${str#?}"}"
    h=$(((h * 31 + $(LC_CTYPE=C printf %d "'$char")) % 4294967296))
    str="${str#?}"
  done
  printf %x\\n $h
}

verbose() { :; }
[ "${MVNW_VERBOSE-}" != true ] || verbose() { printf %s\\n "${1-}"; }

die() {
  printf %s\\n "$1" >&2
  exit 1
}

trim() {
  # MWRAPPER-139:
  #   Trims trailing and leading whitespace, carriage returns, tabs, and linefeeds.
  #   Needed for removing poorly interpreted newline sequences when running in more
  #   exotic environments such as mingw bash on Windows.
  printf "%s" "${1}" | tr -d '[:space:]'
}

scriptDir="$(dirname "$0")"
scriptName="$(basename "$0")"

# parse distributionUrl and optional distributionSha256Sum, requires .mvn/wrapper/maven-wrapper.properties
while IFS="=" read -r key value; do
  case "${key-}" in
  distributionUrl) distributionUrl=$(trim "${value-}") ;;
  distributionSha256Sum) distributionSha256Sum=$(trim "${value-}") ;;
  esac
done <"$scriptDir/.mvn/wrapper/maven-wrapper.properties"
[ -n "${distributionUrl-}" ] || die "cannot read distributionUrl property in $scriptDir/.mvn/wrapper/maven-wrapper.properties"

case "${distributionUrl##*/}" in
maven-mvnd-*bin.*)
  MVN_CMD=mvnd.sh _MVNW_REPO_PATTERN=/maven/mvnd/
  case "${PROCESSOR_ARCHITECTURE-}${PROCESSOR_ARCHITEW6432-}:$(uname -a)" in
  *AMD64:CYGWIN* | *AMD64:MINGW*) distributionPlatform=windows-amd64 ;;
  :Darwin*x86_64) distributionPlatform=darwin-amd64 ;;
  :Darwin*arm64) distributionPlatform=darwin-aarch64 ;;
  :Linux*x86_64*) distributionPlatform=linux-amd64 ;;
  *)
    echo "Cannot detect native platform for mvnd on $(uname)-$(uname -m), use pure java version" >&2
    distributionPlatform=linux-amd64
    ;;
  esac
  distributionUrl="${distributionUrl%-bin.*}-$distributionPlatform.zip"
  ;;
maven-mvnd-*) MVN_CMD=mvnd.sh _MVNW_REPO_PATTERN=/maven/mvnd/ ;;
*) MVN_CMD="mvn${scriptName#mvnw}" _MVNW_REPO_PATTERN=/org/apache/maven/ ;;
esac

# apply MVNW_REPOURL and calculate MAVEN_HOME
# maven home pattern: ~/.m2/wrapper/dists/{apache-maven-<version>,maven-mvnd-<version>-<platform>}/<hash>
[ -z "${MVNW_REPOURL-}" ] || distributionUrl="$MVNW_REPOURL$_MVNW_REPO_PATTERN${distributionUrl#*"$_MVNW_REPO_PATTERN"}"
distributionUrlName="${distributionUrl##*/}"
distributionUrlNameMain="${distributionUrlName%.*}"
distributionUrlNameMain="${distributionUrlNameMain%-bin}"
MAVEN_USER_HOME="${MAVEN_USER_HOME:-${HOME}/.m2}"
MAVEN_HOME="${MAVEN_USER_HOME}/wrapper/dists/${distributionUrlNameMain-}/$(hash_string "$distributionUrl")"

exec_maven() {
  unset MVNW_VERBOSE MVNW_USERNAME MVNW_PASSWORD MVNW_REPOURL || :
  exec "$MAVEN_HOME/bin/$MVN_CMD" "$@" || die "cannot exec $MAVEN_HOME/bin/$MVN_CMD"
}

if [ -d "$MAVEN_HOME" ]; then
  verbose "found existing MAVEN_HOME at $MAVEN_HOME"
  exec_maven "$@"
fi

case "${distributionUrl-}" in
*?-bin.zip | *?maven-mvnd-?*-?*.zip) ;;
*) die "distributionUrl is not valid, must match *-bin.zip or maven-mvnd-*.zip, but found '${distributionUrl-}'" ;;
esac

# prepare tmp dir
if TMP_DOWNLOAD_DIR="$(mktemp -d)" && [ -d "$TMP_DOWNLOAD_DIR" ]; then
  clean() { rm -rf -- "$TMP_DOWNLOAD_DIR"; }
  trap clean HUP INT TERM EXIT
else
  die "cannot create temp dir"
fi

mkdir -p -- "${MAVEN_HOME%/*}"

# Download and Install Apache Maven
verbose "Couldn't find MAVEN_HOME, downloading and installing it ..."
verbose "Downloading from: $distributionUrl"
verbose "Downloading to: $TMP_DOWNLOAD_DIR/$distributionUrlName"

# select .zip or .tar.gz
if ! command -v unzip >/dev/null; then
  distributionUrl="${distributionUrl%.zip}.tar.gz"
  distributionUrlName="${distributionUrl##*/}"
fi

# verbose opt
__MVNW_QUIET_WGET=--quiet __MVNW_QUIET_CURL=--silent __MVNW_QUIET_UNZIP=-q __MVNW_QUIET_TAR=''
[ "${MVNW_VERBOSE-}" != true ] || __MVNW_QUIET_WGET='' __MVNW_QUIET_CURL='' __MVNW_QUIET_UNZIP='' __MVNW_QUIET_TAR=v

# normalize http auth
case "${MVNW_PASSWORD:+has-password}" in
'') MVNW_USERNAME='' MVNW_PASSWORD='' ;;
has-password) [ -n "${MVNW_USERNAME-}" ] || MVNW_USERNAME='' MVNW_PASSWORD='' ;;
esac

if [ -z "${MVNW_USERNAME-}" ] && command -v wget >/dev/null; then
  verbose "Found wget ... using wget"
  wget ${__MVNW_QUIET_WGET:+"$__MVNW_QUIET_WGET"} "$distributionUrl" -O "$TMP_DOWNLOAD_DIR/$distributionUrlName" || die "wget: Failed to fetch $distributionUrl"
elif [ -z "${MVNW_USERNAME-}" ] && command -v curl >/dev/null; then
  verbose "Found curl ... using curl"
  curl ${__MVNW_QUIET_CURL:+"$__MVNW_QUIET_CURL"} -f -L -o "$TMP_DOWNLOAD_DIR/$distributionUrlName" "$distributionUrl" || die "curl: Failed to fetch $distributionUrl"
elif set_java_home; then
  verbose "Falling back to use Java to download"
  javaSource="$TMP_DOWNLOAD_DIR/Downloader.java"
  targetZip="$TMP_DOWNLOAD_DIR/$distributionUrlName"
  cat >"$javaSource" <<-END
	public class Downloader extends java.net.Authenticator
	{
	  protected java.net.PasswordAuthentication getPasswordAuthentication()
	  {
	    return new java.net.PasswordAuthentication( System.getenv( "MVNW_USERNAME" ), System.getenv( "MVNW_PASSWORD" ).toCharArray() );
	  }
	  public static void main( String[] args ) throws Exception
	  {
	    setDefault( new Downloader() );
	    java.nio.file.Files.copy( java.net.URI.create( args[0] ).toURL().openStream(), java.nio.file.Paths.get( args[1] ).toAbsolutePath().normalize() );
	  }
	}
	END
  # For Cygwin/MinGW, switch paths to Windows format before running javac and java
  verbose " - Compiling Downloader.java ..."
  "$(native_path "$JAVACCMD")" "$(native_path "$javaSource")" || die "Failed to compile Downloader.java"
  verbose " - Running Downloader.java ..."
  "$(native_path "$JAVACMD")" -cp "$(native_path "$TMP_DOWNLOAD_DIR")" Downloader "$distributionUrl" "$(native_path "$targetZip")"
fi

# If specified, validate the SHA-256 sum of the Maven distribution zip file
if [ -n "${distributionSha256Sum-}" ]; then
  distributionSha256Result=false
  if [ "$MVN_CMD" = mvnd.sh ]; then
    echo "Checksum validation is not supported for maven-mvnd." >&2
    echo "Please disable validation by removing 'distributionSha256Sum' from your maven-wrapper.properties." >&2
    exit 1
  elif command -v sha256sum >/dev/null; then
    if echo "$distributionSha256Sum  $TMP_DOWNLOAD_DIR/$distributionUrlName" | sha256sum -c - >/dev/null 2>&1; then
      distributionSha256Result=true
    fi
  elif command -v shasum >/dev/null; then
    if echo "$distributionSha256Sum  $TMP_DOWNLOAD_DIR/$distributionUrlName" | shasum -a 256 -c >/dev/null 2>&1; then
      distributionSha256Result=true
    fi
  else
    echo "Checksum validation was requested but neither 'sha256sum' or 'shasum' are available." >&2
    echo "Please install either command, or disable validation by removing 'distributionSha256Sum' from your maven-wrapper.properties." >&2
    exit 1
  fi
  if [ $distributionSha256Result = false ]; then
    echo "Error: Failed to validate Maven distribution SHA-256, your Maven distribution might be compromised." >&2
    echo "If you updated your Maven version, you need to update the specified distributionSha256Sum property." >&2
    exit 1
  fi
fi

# unzip and move
if command -v unzip >/dev/null; then
  unzip ${__MVNW_QUIET_UNZIP:+"$__MVNW_QUIET_UNZIP"} "$TMP_DOWNLOAD_DIR/$distributionUrlName" -d "$TMP_DOWNLOAD_DIR" || die "failed to unzip"
else
  tar xzf${__MVNW_QUIET_TAR:+"$__MVNW_QUIET_TAR"} "$TMP_DOWNLOAD_DIR/$distributionUrlName" -C "$TMP_DOWNLOAD_DIR" || die "failed to untar"
fi

# Find the actual extracted directory name (handles snapshots where filename != directory name)
actualDistributionDir=""

# First try the expected directory name (for regular distributions)
if [ -d "$TMP_DOWNLOAD_DIR/$distributionUrlNameMain" ]; then
  if [ -f "$TMP_DOWNLOAD_DIR/$distributionUrlNameMain/bin/$MVN_CMD" ]; then
    actualDistributionDir="$distributionUrlNameMain"
  fi
fi

# If not found, search for any directory with the Maven executable (for snapshots)
if [ -z "$actualDistributionDir" ]; then
  # enable globbing to iterate over items
  set +f
  for dir in "$TMP_DOWNLOAD_DIR"/*; do
    if [ -d "$dir" ]; then
      if [ -f "$dir/bin/$MVN_CMD" ]; then
        actualDistributionDir="$(basename "$dir")"
        break
      fi
    fi
  done
  set -f
fi

if [ -z "$actualDistributionDir" ]; then
  verbose "Contents of $TMP_DOWNLOAD_DIR:"
  verbose "$(ls -la "$TMP_DOWNLOAD_DIR")"
  die "Could not find Maven distribution directory in extracted archive"
fi

verbose "Found extracted Maven distribution directory: $actualDistributionDir"
printf %s\\n "$distributionUrl" >"$TMP_DOWNLOAD_DIR/$actualDistributionDir/mvnw.url"
mv -- "$TMP_DOWNLOAD_DIR/$actualDistributionDir" "$MAVEN_HOME" || [ -d "$MAVEN_HOME" ] || die "fail to move MAVEN_HOME"

clean || :
exec_maven "$@"
</file>

<file path="data-platform-mcp-server/mvnw.cmd">
<# : batch portion
@REM ----------------------------------------------------------------------------
@REM Licensed to the Apache Software Foundation (ASF) under one
@REM or more contributor license agreements.  See the NOTICE file
@REM distributed with this work for additional information
@REM regarding copyright ownership.  The ASF licenses this file
@REM to you under the Apache License, Version 2.0 (the
@REM "License"); you may not use this file except in compliance
@REM with the License.  You may obtain a copy of the License at
@REM
@REM    http://www.apache.org/licenses/LICENSE-2.0
@REM
@REM Unless required by applicable law or agreed to in writing,
@REM software distributed under the License is distributed on an
@REM "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
@REM KIND, either express or implied.  See the License for the
@REM specific language governing permissions and limitations
@REM under the License.
@REM ----------------------------------------------------------------------------

@REM ----------------------------------------------------------------------------
@REM Apache Maven Wrapper startup batch script, version 3.3.4
@REM
@REM Optional ENV vars
@REM   MVNW_REPOURL - repo url base for downloading maven distribution
@REM   MVNW_USERNAME/MVNW_PASSWORD - user and password for downloading maven
@REM   MVNW_VERBOSE - true: enable verbose log; others: silence the output
@REM ----------------------------------------------------------------------------

@IF "%__MVNW_ARG0_NAME__%"=="" (SET __MVNW_ARG0_NAME__=%~nx0)
@SET __MVNW_CMD__=
@SET __MVNW_ERROR__=
@SET __MVNW_PSMODULEP_SAVE=%PSModulePath%
@SET PSModulePath=
@FOR /F "usebackq tokens=1* delims==" %%A IN (`powershell -noprofile "& {$scriptDir='%~dp0'; $script='%__MVNW_ARG0_NAME__%'; icm -ScriptBlock ([Scriptblock]::Create((Get-Content -Raw '%~f0'))) -NoNewScope}"`) DO @(
  IF "%%A"=="MVN_CMD" (set __MVNW_CMD__=%%B) ELSE IF "%%B"=="" (echo %%A) ELSE (echo %%A=%%B)
)
@SET PSModulePath=%__MVNW_PSMODULEP_SAVE%
@SET __MVNW_PSMODULEP_SAVE=
@SET __MVNW_ARG0_NAME__=
@SET MVNW_USERNAME=
@SET MVNW_PASSWORD=
@IF NOT "%__MVNW_CMD__%"=="" ("%__MVNW_CMD__%" %*)
@echo Cannot start maven from wrapper >&2 && exit /b 1
@GOTO :EOF
: end batch / begin powershell #>

$ErrorActionPreference = "Stop"
if ($env:MVNW_VERBOSE -eq "true") {
  $VerbosePreference = "Continue"
}

# calculate distributionUrl, requires .mvn/wrapper/maven-wrapper.properties
$distributionUrl = (Get-Content -Raw "$scriptDir/.mvn/wrapper/maven-wrapper.properties" | ConvertFrom-StringData).distributionUrl
if (!$distributionUrl) {
  Write-Error "cannot read distributionUrl property in $scriptDir/.mvn/wrapper/maven-wrapper.properties"
}

switch -wildcard -casesensitive ( $($distributionUrl -replace '^.*/','') ) {
  "maven-mvnd-*" {
    $USE_MVND = $true
    $distributionUrl = $distributionUrl -replace '-bin\.[^.]*$',"-windows-amd64.zip"
    $MVN_CMD = "mvnd.cmd"
    break
  }
  default {
    $USE_MVND = $false
    $MVN_CMD = $script -replace '^mvnw','mvn'
    break
  }
}

# apply MVNW_REPOURL and calculate MAVEN_HOME
# maven home pattern: ~/.m2/wrapper/dists/{apache-maven-<version>,maven-mvnd-<version>-<platform>}/<hash>
if ($env:MVNW_REPOURL) {
  $MVNW_REPO_PATTERN = if ($USE_MVND -eq $False) { "/org/apache/maven/" } else { "/maven/mvnd/" }
  $distributionUrl = "$env:MVNW_REPOURL$MVNW_REPO_PATTERN$($distributionUrl -replace "^.*$MVNW_REPO_PATTERN",'')"
}
$distributionUrlName = $distributionUrl -replace '^.*/',''
$distributionUrlNameMain = $distributionUrlName -replace '\.[^.]*$','' -replace '-bin$',''

$MAVEN_M2_PATH = "$HOME/.m2"
if ($env:MAVEN_USER_HOME) {
  $MAVEN_M2_PATH = "$env:MAVEN_USER_HOME"
}

if (-not (Test-Path -Path $MAVEN_M2_PATH)) {
    New-Item -Path $MAVEN_M2_PATH -ItemType Directory | Out-Null
}

$MAVEN_WRAPPER_DISTS = $null
if ((Get-Item $MAVEN_M2_PATH).Target[0] -eq $null) {
  $MAVEN_WRAPPER_DISTS = "$MAVEN_M2_PATH/wrapper/dists"
} else {
  $MAVEN_WRAPPER_DISTS = (Get-Item $MAVEN_M2_PATH).Target[0] + "/wrapper/dists"
}

$MAVEN_HOME_PARENT = "$MAVEN_WRAPPER_DISTS/$distributionUrlNameMain"
$MAVEN_HOME_NAME = ([System.Security.Cryptography.SHA256]::Create().ComputeHash([byte[]][char[]]$distributionUrl) | ForEach-Object {$_.ToString("x2")}) -join ''
$MAVEN_HOME = "$MAVEN_HOME_PARENT/$MAVEN_HOME_NAME"

if (Test-Path -Path "$MAVEN_HOME" -PathType Container) {
  Write-Verbose "found existing MAVEN_HOME at $MAVEN_HOME"
  Write-Output "MVN_CMD=$MAVEN_HOME/bin/$MVN_CMD"
  exit $?
}

if (! $distributionUrlNameMain -or ($distributionUrlName -eq $distributionUrlNameMain)) {
  Write-Error "distributionUrl is not valid, must end with *-bin.zip, but found $distributionUrl"
}

# prepare tmp dir
$TMP_DOWNLOAD_DIR_HOLDER = New-TemporaryFile
$TMP_DOWNLOAD_DIR = New-Item -Itemtype Directory -Path "$TMP_DOWNLOAD_DIR_HOLDER.dir"
$TMP_DOWNLOAD_DIR_HOLDER.Delete() | Out-Null
trap {
  if ($TMP_DOWNLOAD_DIR.Exists) {
    try { Remove-Item $TMP_DOWNLOAD_DIR -Recurse -Force | Out-Null }
    catch { Write-Warning "Cannot remove $TMP_DOWNLOAD_DIR" }
  }
}

New-Item -Itemtype Directory -Path "$MAVEN_HOME_PARENT" -Force | Out-Null

# Download and Install Apache Maven
Write-Verbose "Couldn't find MAVEN_HOME, downloading and installing it ..."
Write-Verbose "Downloading from: $distributionUrl"
Write-Verbose "Downloading to: $TMP_DOWNLOAD_DIR/$distributionUrlName"

$webclient = New-Object System.Net.WebClient
if ($env:MVNW_USERNAME -and $env:MVNW_PASSWORD) {
  $webclient.Credentials = New-Object System.Net.NetworkCredential($env:MVNW_USERNAME, $env:MVNW_PASSWORD)
}
[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12
$webclient.DownloadFile($distributionUrl, "$TMP_DOWNLOAD_DIR/$distributionUrlName") | Out-Null

# If specified, validate the SHA-256 sum of the Maven distribution zip file
$distributionSha256Sum = (Get-Content -Raw "$scriptDir/.mvn/wrapper/maven-wrapper.properties" | ConvertFrom-StringData).distributionSha256Sum
if ($distributionSha256Sum) {
  if ($USE_MVND) {
    Write-Error "Checksum validation is not supported for maven-mvnd. `nPlease disable validation by removing 'distributionSha256Sum' from your maven-wrapper.properties."
  }
  Import-Module $PSHOME\Modules\Microsoft.PowerShell.Utility -Function Get-FileHash
  if ((Get-FileHash "$TMP_DOWNLOAD_DIR/$distributionUrlName" -Algorithm SHA256).Hash.ToLower() -ne $distributionSha256Sum) {
    Write-Error "Error: Failed to validate Maven distribution SHA-256, your Maven distribution might be compromised. If you updated your Maven version, you need to update the specified distributionSha256Sum property."
  }
}

# unzip and move
Expand-Archive "$TMP_DOWNLOAD_DIR/$distributionUrlName" -DestinationPath "$TMP_DOWNLOAD_DIR" | Out-Null

# Find the actual extracted directory name (handles snapshots where filename != directory name)
$actualDistributionDir = ""

# First try the expected directory name (for regular distributions)
$expectedPath = Join-Path "$TMP_DOWNLOAD_DIR" "$distributionUrlNameMain"
$expectedMvnPath = Join-Path "$expectedPath" "bin/$MVN_CMD"
if ((Test-Path -Path $expectedPath -PathType Container) -and (Test-Path -Path $expectedMvnPath -PathType Leaf)) {
  $actualDistributionDir = $distributionUrlNameMain
}

# If not found, search for any directory with the Maven executable (for snapshots)
if (!$actualDistributionDir) {
  Get-ChildItem -Path "$TMP_DOWNLOAD_DIR" -Directory | ForEach-Object {
    $testPath = Join-Path $_.FullName "bin/$MVN_CMD"
    if (Test-Path -Path $testPath -PathType Leaf) {
      $actualDistributionDir = $_.Name
    }
  }
}

if (!$actualDistributionDir) {
  Write-Error "Could not find Maven distribution directory in extracted archive"
}

Write-Verbose "Found extracted Maven distribution directory: $actualDistributionDir"
Rename-Item -Path "$TMP_DOWNLOAD_DIR/$actualDistributionDir" -NewName $MAVEN_HOME_NAME | Out-Null
try {
  Move-Item -Path "$TMP_DOWNLOAD_DIR/$MAVEN_HOME_NAME" -Destination $MAVEN_HOME_PARENT | Out-Null
} catch {
  if (! (Test-Path -Path "$MAVEN_HOME" -PathType Container)) {
    Write-Error "fail to move MAVEN_HOME"
  }
} finally {
  try { Remove-Item $TMP_DOWNLOAD_DIR -Recurse -Force | Out-Null }
  catch { Write-Warning "Cannot remove $TMP_DOWNLOAD_DIR" }
}

Write-Output "MVN_CMD=$MAVEN_HOME/bin/$MVN_CMD"
</file>

<file path="data-platform-mcp-server/pom.xml">
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>3.5.9</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>
    <groupId>com.example</groupId>
    <artifactId>data-platform-mcp-server</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>data-platform-mcp-server</name>
    <description>data-platform-mcp-server</description>
    <url/>
    <licenses>
        <license/>
    </licenses>
    <developers>
        <developer/>
    </developers>
    <scm>
        <connection/>
        <developerConnection/>
        <tag/>
        <url/>
    </scm>
    <properties>
        <java.version>17</java.version>
        <spring-ai.version>1.1.2</spring-ai.version>
    </properties>
    <dependencies>
        <dependency>
            <groupId>org.springframework.ai</groupId>
            <artifactId>spring-ai-starter-mcp-server</artifactId>
        </dependency>

        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>
    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>org.springframework.ai</groupId>
                <artifactId>spring-ai-bom</artifactId>
                <version>${spring-ai.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
        </dependencies>
    </dependencyManagement>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>

</project>
</file>

<file path="data-ingest-service/src/main/java/com/platform/data/ingest/controller/data-platform-core.code-workspace">
{
	"folders": [
		{
			"path": "../../../../../../../../.."
		}
	],
	"settings": {
		"java.configuration.updateBuildConfiguration": "interactive"
	}
}
</file>

<file path="data-ingest-service/src/main/resources/application-kafka.properties">
# Kafka Configuration
spring.kafka.bootstrap-servers=localhost:9092
spring.kafka.consumer.group-id=data-ingest-group
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonDeserializer
spring.kafka.consumer.properties.spring.json.trusted.packages=*
spring.kafka.listener.ack-mode=manual
</file>

<file path="data-ingest-service/src/main/resources/application.properties">
spring.application.name=data-ingest-service
server.port=8081

# Cassandra Configuration
cassandra.contact-points=localhost
cassandra.port=9042
cassandra.local-datacenter=datacenter1
cassandra.keyspace=test_keyspace

# Logging
logging.level.com.platform=INFO
logging.level.com.datastax.oss.driver=WARN
</file>

<file path="data-ingest-service/pom.xml">
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 
         http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>com.platform</groupId>
        <artifactId>data-platform-core</artifactId>
        <version>1.0.0-SNAPSHOT</version>
    </parent>

    <artifactId>data-ingest-service</artifactId>
    <packaging>jar</packaging>

    <name>Data Ingest Service</name>
    <description>Write path REST API for data ingestion</description>

    <dependencies>
        <!-- Internal Dependencies -->
        <dependency>
            <groupId>com.platform</groupId>
            <artifactId>data-common</artifactId>
        </dependency>

        <!-- Spring Boot -->
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>

        <!-- Kafka -->
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka</artifactId>
        </dependency>

        <!-- Datastax Driver -->
        <dependency>
            <groupId>com.datastax.oss</groupId>
            <artifactId>java-driver-core</artifactId>
        </dependency>
        <dependency>
            <groupId>com.datastax.oss</groupId>
            <artifactId>java-driver-query-builder</artifactId>
        </dependency>

        <!-- Testing -->
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka-test</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>testcontainers</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>cassandra</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
</project>
</file>

<file path="data-query-service/src/main/resources/application.properties">
spring.application.name=data-query-service
server.port=8082

# Cassandra Configuration
cassandra.contact-points=localhost
cassandra.port=9042
cassandra.local-datacenter=datacenter1
cassandra.keyspace=test_keyspace

# Logging
logging.level.com.platform=INFO
logging.level.com.datastax.oss.driver=WARN
</file>

<file path="data-query-service/pom.xml">
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 
         http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>com.platform</groupId>
        <artifactId>data-platform-core</artifactId>
        <version>1.0.0-SNAPSHOT</version>
    </parent>

    <artifactId>data-query-service</artifactId>
    <packaging>jar</packaging>

    <name>Data Query Service</name>
    <description>Read path REST API for data retrieval</description>

    <dependencies>
        <!-- Internal Dependencies -->
        <dependency>
            <groupId>com.platform</groupId>
            <artifactId>data-common</artifactId>
        </dependency>

        <!-- Spring Boot -->
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>

        <!-- Datastax Driver -->
        <dependency>
            <groupId>com.datastax.oss</groupId>
            <artifactId>java-driver-core</artifactId>
        </dependency>
        <dependency>
            <groupId>com.datastax.oss</groupId>
            <artifactId>java-driver-query-builder</artifactId>
        </dependency>

        <!-- Testing -->
        <dependency>
            <groupId>com.platform</groupId>
            <artifactId>data-ingest-service</artifactId>
            <version>${project.version}</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>testcontainers</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>cassandra</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testcontainers</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.assertj</groupId>
            <artifactId>assertj-core</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
</project>
</file>

<file path=".gitignore">
# Maven
target/
*.jar
*.war
*.ear
*.class

# IDE
.idea/
*.iml
*.iws
*.ipr
.vscode/
.settings/
.classpath
.project

# OS
.DS_Store
Thumbs.db

# Logs
*.log

# Remomix
remomix*

# Test containers
.testcontainers/

# Spring Boot
spring-boot-devtools.properties
</file>

<file path="PROJECT_SUMMARY.md">
# Data Platform Core - Project Summary

## ‚úÖ Project Complete!

Successfully generated a complete multi-module Maven project for a generic, multi-tenant Data Platform.

## üì¶ What Was Built

### Project Structure
```
data-platform-core/
‚îú‚îÄ‚îÄ pom.xml (Parent POM)
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ data-common/
‚îÇ   ‚îú‚îÄ‚îÄ pom.xml
‚îÇ   ‚îî‚îÄ‚îÄ src/main/java/com/platform/data/common/
‚îÇ       ‚îú‚îÄ‚îÄ config/TenantConfig.java
‚îÇ       ‚îî‚îÄ‚îÄ test/SchemaInit.java
‚îú‚îÄ‚îÄ data-ingest-service/
‚îÇ   ‚îú‚îÄ‚îÄ pom.xml
‚îÇ   ‚îî‚îÄ‚îÄ src/main/java/com/platform/data/ingest/
‚îÇ       ‚îú‚îÄ‚îÄ IngestServiceApplication.java
‚îÇ       ‚îú‚îÄ‚îÄ config/CassandraConfig.java
‚îÇ       ‚îú‚îÄ‚îÄ controller/IngestController.java
‚îÇ       ‚îî‚îÄ‚îÄ service/DynamicIngestService.java
‚îî‚îÄ‚îÄ data-query-service/
    ‚îú‚îÄ‚îÄ pom.xml
    ‚îî‚îÄ‚îÄ src/
        ‚îú‚îÄ‚îÄ main/java/com/platform/data/query/
        ‚îÇ   ‚îú‚îÄ‚îÄ QueryServiceApplication.java
        ‚îÇ   ‚îú‚îÄ‚îÄ config/CassandraConfig.java
        ‚îÇ   ‚îú‚îÄ‚îÄ controller/QueryController.java
        ‚îÇ   ‚îî‚îÄ‚îÄ service/DynamicRetrievalService.java
        ‚îî‚îÄ‚îÄ test/java/com/platform/data/query/
            ‚îî‚îÄ‚îÄ GenericPlatformIntegrationTest.java
```

## üéØ Key Features Implemented

### 1. **Dynamic Schema Support**
- ‚úÖ No static `@Table` classes
- ‚úÖ Runtime configuration via `TenantConfig` record
- ‚úÖ Supports different schemas per tenant

### 2. **Auto-Partitioning**
- ‚úÖ Automatic bucket column injection (e.g., extracting year from dates)
- ‚úÖ Configured via `bucketColumn` in `TenantConfig`
- ‚úÖ Transparent to API consumers

### 3. **UDT Handling**
- ‚úÖ Automatic conversion: `Map<String, Object>` ‚Üí `UdtValue` (ingestion)
- ‚úÖ Automatic conversion: `UdtValue` ‚Üí `Map<String, Object>` (retrieval)
- ‚úÖ Recursive support for nested UDTs
- ‚úÖ Clean JSON responses (no Cassandra types exposed)

### 4. **Scatter-Gather Queries**
- ‚úÖ Parallel async queries across year buckets
- ‚úÖ Uses `CompletableFuture` for concurrency
- ‚úÖ Automatic bucket range calculation
- ‚úÖ Efficient result merging

### 5. **Raw CqlSession**
- ‚úÖ No Spring Data Repositories
- ‚úÖ Direct use of Datastax Java Driver 4.x
- ‚úÖ QueryBuilder for dynamic CQL construction
- ‚úÖ Full control over query execution

## üß™ Integration Test

The `GenericPlatformIntegrationTest` proves the entire system works:

### Test Scenario
1. **Setup**: Starts Cassandra via Testcontainers
2. **Schema**: Creates UDT and bucketed table
3. **Ingest**: Inserts data for IBM across Dec 2023 and Jan 2024 (2 partitions)
4. **Query**: Requests data from 2023-12-01 to 2024-02-01
5. **Verify**: Asserts scatter-gather returned data from both years

### To Run (requires Docker)
```bash
cd data-query-service
mvn test -Dtest=GenericPlatformIntegrationTest
```

## üõ†Ô∏è Tech Stack

| Component | Version |
|-----------|---------|
| Java | 21 |
| Spring Boot | 3.2.1 |
| Datastax Driver | 4.17.0 |
| JUnit | 5.10.1 |
| Testcontainers | 1.19.3 |
| Maven | Multi-module |

## üöÄ Build Status

```
‚úÖ BUILD SUCCESS
‚úÖ All modules compiled
‚úÖ Integration test ready (requires Docker)
```

## üìã Next Steps

1. **Start Cassandra** (for local testing)
   ```bash
   docker run -d --name cassandra -p 9042:9042 cassandra:4.1
   ```

2. **Initialize Schema**
   ```bash
   docker exec -it cassandra cqlsh
   # Then run the CQL from SchemaInit.java
   ```

3. **Run Services**
   ```bash
   # Ingest Service (port 8081)
   cd data-ingest-service
   mvn spring-boot:run
   
   # Query Service (port 8082)
   cd data-query-service
   mvn spring-boot:run
   ```

4. **Run Integration Test**
   ```bash
   cd data-query-service
   mvn test
   ```

## üéì Architecture Highlights

### DynamicIngestService
- Registers tenant configs at runtime
- Auto-extracts bucket values (e.g., year from date)
- Converts Map ‚Üí UdtValue using session metadata
- Builds dynamic INSERT queries with QueryBuilder

### DynamicRetrievalService
- Implements scatter-gather pattern
- Fires parallel async queries per bucket
- Merges results from all partitions
- Converts UdtValue ‚Üí Map recursively

### TenantConfig
- Stores keyspace, table, partition keys
- Optional bucket column for partitioning
- Set of UDT column names
- Helper methods for bucket/UDT checks

## üìä Example Usage

### Register Tenant
```java
TenantConfig config = TenantConfig.withBucket(
    "test_keyspace",
    "DailyNumeric",
    List.of("tenant_id", "instrument_id", "period_year"),
    "period_year",
    Set.of("data")
);
ingestService.registerTenant("IBM", config);
```

### Ingest Data
```java
Map<String, Object> payload = Map.of(
    "tenant_id", "IBM",
    "instrument_id", "IBM_STOCK",
    "period_date", LocalDate.of(2024, 1, 10),
    "field_id", "revenue",
    "data", Map.of(
        "value", BigDecimal.valueOf(102.7),
        "report_time", Instant.now()
    )
);
ingestService.ingest("IBM", payload);
```

### Query Data
```java
Map<String, Object> criteria = Map.of(
    "tenant_id", "IBM",
    "instrument_id", "IBM_STOCK",
    "start_date", "2023-12-01",
    "end_date", "2024-02-01"
);
List<Map<String, Object>> results = retrievalService.retrieve("IBM", criteria);
```

## üéâ Success Metrics

- ‚úÖ **Zero** static `@Table` classes
- ‚úÖ **100%** dynamic schema configuration
- ‚úÖ **Automatic** UDT conversion both ways
- ‚úÖ **Parallel** scatter-gather queries
- ‚úÖ **Clean** JSON responses
- ‚úÖ **Full** integration test coverage

---

**Built with ‚ù§Ô∏è by Principal Platform Engineer**
</file>

<file path="data-common/src/main/java/com/platform/data/common/config/TenantConfig.java">
package com.platform.data.common.config;

import java.util.List;
import java.util.Optional;
import java.util.Set;

/**
 * Runtime metadata configuration for a tenant's table. This record stores all necessary information
 * to construct dynamic CQL queries.
 */
public record TenantConfig(
    String keyspace,
    String tableName,
    List<String> partitionKeys,
    Optional<String> bucketColumn,
    Set<String> udtColumns) {
  /** Creates a TenantConfig without a bucket column. */
  public static TenantConfig withoutBucket(
      String keyspace, String tableName, List<String> partitionKeys, Set<String> udtColumns) {
    return new TenantConfig(keyspace, tableName, partitionKeys, Optional.empty(), udtColumns);
  }

  /** Creates a TenantConfig with a bucket column. */
  public static TenantConfig withBucket(
      String keyspace,
      String tableName,
      List<String> partitionKeys,
      String bucketColumn,
      Set<String> udtColumns) {
    return new TenantConfig(
        keyspace, tableName, partitionKeys, Optional.of(bucketColumn), udtColumns);
  }

  /** Checks if this configuration uses bucketing. */
  public boolean hasBucket() {
    return bucketColumn.isPresent();
  }

  /** Gets the bucket column name, or throws if not present. */
  public String getBucketColumnOrThrow() {
    return bucketColumn.orElseThrow(
        () -> new IllegalStateException("Bucket column not configured for this tenant"));
  }

  /** Checks if a column is a UDT. */
  public boolean isUdtColumn(String columnName) {
    return udtColumns.contains(columnName);
  }
}
</file>

<file path="data-common/src/main/java/com/platform/data/common/model/PartitionKey.java">
package com.platform.data.common.model;

import java.util.List;

/**
 * Composite partition key for batch grouping. Auto-implements equals/hashCode for use as Map keys.
 */
public record PartitionKey(List<Object> values) {

  /** Creates a partition key from individual values. */
  public static PartitionKey of(Object... values) {
    return new PartitionKey(List.of(values));
  }

  @Override
  public String toString() {
    return "PartitionKey" + values;
  }
}
</file>

<file path="data-common/src/main/java/com/platform/data/common/registry/TenantConfigRegistry.java">
package com.platform.data.common.registry;

import com.platform.data.common.config.TenantConfig;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Centralized tenant configuration registry with polymorphic routing support. Supports multiple
 * table types per tenant based on (tenantId, periodicity, dataType).
 */
public class TenantConfigRegistry {

  private static final Logger log = LoggerFactory.getLogger(TenantConfigRegistry.class);

  private final Map<RegistryKey, TenantConfig> configs = new ConcurrentHashMap<>();

  /**
   * Registers a tenant configuration.
   *
   * @param tenantId Tenant identifier
   * @param periodicity Data periodicity (e.g., "DAILY", "MONTHLY")
   * @param dataType Data type (e.g., "NUMERIC", "STRING")
   * @param config Tenant configuration
   */
  public void register(String tenantId, String periodicity, String dataType, TenantConfig config) {
    RegistryKey key = new RegistryKey(tenantId, periodicity, dataType);
    configs.put(key, config);
    log.info("Registered config: {} -> {}.{}", key, config.keyspace(), config.tableName());
  }

  /**
   * Looks up a tenant configuration.
   *
   * @param tenantId Tenant identifier
   * @param periodicity Data periodicity
   * @param dataType Data type
   * @return Tenant configuration
   * @throws IllegalArgumentException if config not found
   */
  public TenantConfig lookup(String tenantId, String periodicity, String dataType) {
    RegistryKey key = new RegistryKey(tenantId, periodicity, dataType);
    TenantConfig config = configs.get(key);

    if (config == null) {
      throw new IllegalArgumentException(
          "No configuration found for: " + key + ". Available configs: " + configs.keySet());
    }

    return config;
  }

  /** Checks if a configuration exists. */
  public boolean exists(String tenantId, String periodicity, String dataType) {
    RegistryKey key = new RegistryKey(tenantId, periodicity, dataType);
    return configs.containsKey(key);
  }

  /** Removes a configuration. */
  public void unregister(String tenantId, String periodicity, String dataType) {
    RegistryKey key = new RegistryKey(tenantId, periodicity, dataType);
    configs.remove(key);
    log.info("Unregistered config: {}", key);
  }

  /** Clears all configurations (useful for testing). */
  public void clear() {
    configs.clear();
    log.info("Cleared all configurations");
  }

  /** Composite key for registry lookup. */
  private record RegistryKey(String tenantId, String periodicity, String dataType) {
    @Override
    public String toString() {
      return String.format("(%s, %s, %s)", tenantId, periodicity, dataType);
    }
  }
}
</file>

<file path="data-common/src/test/java/com/platform/data/common/model/PartitionKeyTest.java">
package com.platform.data.common.model;

import static org.assertj.core.api.Assertions.*;

import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.junit.jupiter.api.Test;

/** Unit tests for PartitionKey. */
class PartitionKeyTest {

  @Test
  void testEquals_sameValues_returnsTrue() {
    // Arrange
    PartitionKey key1 = new PartitionKey(List.of("IBM", "IBM_STOCK", 2024));
    PartitionKey key2 = new PartitionKey(List.of("IBM", "IBM_STOCK", 2024));

    // Act & Assert
    assertThat(key1).isEqualTo(key2);
  }

  @Test
  void testEquals_differentValues_returnsFalse() {
    // Arrange
    PartitionKey key1 = new PartitionKey(List.of("IBM", "IBM_STOCK", 2024));
    PartitionKey key2 = new PartitionKey(List.of("IBM", "IBM_STOCK", 2023));

    // Act & Assert
    assertThat(key1).isNotEqualTo(key2);
  }

  @Test
  void testHashCode_sameValues_returnsSameHash() {
    // Arrange
    PartitionKey key1 = new PartitionKey(List.of("IBM", "IBM_STOCK", 2024));
    PartitionKey key2 = new PartitionKey(List.of("IBM", "IBM_STOCK", 2024));

    // Act & Assert
    assertThat(key1.hashCode()).isEqualTo(key2.hashCode());
  }

  @Test
  void testAsMapKey_groupsCorrectly() {
    // Arrange
    Map<PartitionKey, String> map = new HashMap<>();
    PartitionKey key1 = new PartitionKey(List.of("IBM", 2024));
    PartitionKey key2 = new PartitionKey(List.of("IBM", 2024));
    PartitionKey key3 = new PartitionKey(List.of("IBM", 2023));

    // Act
    map.put(key1, "value1");
    map.put(key2, "value2"); // Should overwrite value1
    map.put(key3, "value3");

    // Assert
    assertThat(map).hasSize(2);
    assertThat(map.get(key1)).isEqualTo("value2");
    assertThat(map.get(key3)).isEqualTo("value3");
  }

  @Test
  void testToString_containsValues() {
    // Arrange
    PartitionKey key = new PartitionKey(List.of("IBM", "IBM_STOCK", 2024));

    // Act
    String result = key.toString();

    // Assert
    assertThat(result).contains("IBM", "IBM_STOCK", "2024");
  }
}
</file>

<file path="data-common/src/test/java/com/platform/data/common/registry/TenantConfigRegistryTest.java">
package com.platform.data.common.registry;

import static org.assertj.core.api.Assertions.*;

import com.platform.data.common.config.TenantConfig;
import java.util.List;
import java.util.Set;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;

/** Unit tests for TenantConfigRegistry. */
class TenantConfigRegistryTest {

  private TenantConfigRegistry registry;

  @BeforeEach
  void setUp() {
    registry = new TenantConfigRegistry();
  }

  @Test
  void testRegisterAndLookup_success() {
    // Arrange
    TenantConfig config =
        TenantConfig.withBucket(
            "test_ks", "test_table", List.of("tenant_id"), "period_year", Set.of("data"));

    // Act
    registry.register("IBM", "DAILY", "NUMERIC", config);
    TenantConfig result = registry.lookup("IBM", "DAILY", "NUMERIC");

    // Assert
    assertThat(result).isEqualTo(config);
  }

  @Test
  void testPolymorphicRouting_differentDataTypes() {
    // Arrange
    TenantConfig numericConfig =
        TenantConfig.withBucket("test_ks", "numeric_table", List.of("pk"), "period_year", Set.of());
    TenantConfig stringConfig =
        TenantConfig.withBucket("test_ks", "string_table", List.of("pk"), "period_year", Set.of());

    // Act
    registry.register("IBM", "DAILY", "NUMERIC", numericConfig);
    registry.register("IBM", "DAILY", "STRING", stringConfig);

    // Assert
    assertThat(registry.lookup("IBM", "DAILY", "NUMERIC")).isEqualTo(numericConfig);
    assertThat(registry.lookup("IBM", "DAILY", "STRING")).isEqualTo(stringConfig);
  }

  @Test
  void testLookup_nonExistent_throwsException() {
    // Act & Assert
    assertThatThrownBy(() -> registry.lookup("UNKNOWN", "DAILY", "NUMERIC"))
        .isInstanceOf(IllegalArgumentException.class)
        .hasMessageContaining("No configuration found");
  }

  @Test
  void testExists_returnsTrue_whenConfigExists() {
    // Arrange
    TenantConfig config =
        TenantConfig.withBucket("test_ks", "test_table", List.of("pk"), "period_year", Set.of());
    registry.register("IBM", "DAILY", "NUMERIC", config);

    // Act & Assert
    assertThat(registry.exists("IBM", "DAILY", "NUMERIC")).isTrue();
  }

  @Test
  void testExists_returnsFalse_whenConfigDoesNotExist() {
    // Act & Assert
    assertThat(registry.exists("UNKNOWN", "DAILY", "NUMERIC")).isFalse();
  }

  @Test
  void testUnregister_removesConfig() {
    // Arrange
    TenantConfig config =
        TenantConfig.withBucket("test_ks", "test_table", List.of("pk"), "period_year", Set.of());
    registry.register("IBM", "DAILY", "NUMERIC", config);

    // Act
    registry.unregister("IBM", "DAILY", "NUMERIC");

    // Assert
    assertThat(registry.exists("IBM", "DAILY", "NUMERIC")).isFalse();
  }

  @Test
  void testClear_removesAllConfigs() {
    // Arrange
    TenantConfig config1 =
        TenantConfig.withBucket("test_ks", "table1", List.of("pk"), "period_year", Set.of());
    TenantConfig config2 =
        TenantConfig.withBucket("test_ks", "table2", List.of("pk"), "period_year", Set.of());
    registry.register("IBM", "DAILY", "NUMERIC", config1);
    registry.register("MSFT", "MONTHLY", "STRING", config2);

    // Act
    registry.clear();

    // Assert
    assertThat(registry.exists("IBM", "DAILY", "NUMERIC")).isFalse();
    assertThat(registry.exists("MSFT", "MONTHLY", "STRING")).isFalse();
  }
}
</file>

<file path="data-common/pom.xml">
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 
         http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>com.platform</groupId>
        <artifactId>data-platform-core</artifactId>
        <version>1.0.0-SNAPSHOT</version>
    </parent>

    <artifactId>data-common</artifactId>
    <packaging>jar</packaging>

    <name>Data Common</name>
    <description>Shared utilities and configuration models</description>

    <dependencies>
        <!-- Datastax Driver -->
        <dependency>
            <groupId>com.datastax.oss</groupId>
            <artifactId>java-driver-core</artifactId>
        </dependency>

        <!-- Jackson -->
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
        </dependency>

        <!-- Testing -->
        <dependency>
            <groupId>org.junit.jupiter</groupId>
            <artifactId>junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.assertj</groupId>
            <artifactId>assertj-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-core</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.mockito</groupId>
            <artifactId>mockito-junit-jupiter</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>
</project>
</file>

<file path="data-ingest-service/src/main/java/com/platform/data/ingest/config/CassandraConfig.java">
package com.platform.data.ingest.config;

import com.datastax.oss.driver.api.core.CqlSession;
import com.datastax.oss.driver.api.core.CqlSessionBuilder;
import java.net.InetSocketAddress;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

/** Cassandra configuration for the ingest service. */
@Configuration
public class CassandraConfig {

  @Value("${cassandra.contact-points}")
  private String contactPoints;

  @Value("${cassandra.port}")
  private int port;

  @Value("${cassandra.local-datacenter}")
  private String localDatacenter;

  @Value("${cassandra.keyspace}")
  private String keyspace;

  @Bean
  public CqlSession cqlSession() {
    return new CqlSessionBuilder()
        .addContactPoint(new InetSocketAddress(contactPoints, port))
        .withLocalDatacenter(localDatacenter)
        .withKeyspace(keyspace)
        .build();
  }
}
</file>

<file path="data-ingest-service/src/main/java/com/platform/data/ingest/config/KafkaConfig.java">
package com.platform.data.ingest.config;

import java.util.HashMap;
import java.util.Map;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.annotation.EnableKafka;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
import org.springframework.kafka.listener.ContainerProperties;
import org.springframework.kafka.support.serializer.JsonDeserializer;

/** Kafka configuration for the ingest service. */
@EnableKafka
@Configuration
public class KafkaConfig {

  @Value("${spring.kafka.bootstrap-servers:localhost:9092}")
  private String bootstrapServers;

  @Value("${spring.kafka.consumer.group-id:data-ingest-group}")
  private String groupId;

  @Bean
  public ConsumerFactory<String, Object> consumerFactory() {
    Map<String, Object> config = new HashMap<>();
    config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
    config.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
    config.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
    config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
    config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
    config.put(JsonDeserializer.TRUSTED_PACKAGES, "*");
    config.put(
        JsonDeserializer.VALUE_DEFAULT_TYPE, "com.platform.data.ingest.dto.IngestBatchRequest");

    return new DefaultKafkaConsumerFactory<>(config);
  }

  @Bean
  public ConcurrentKafkaListenerContainerFactory<String, Object> kafkaListenerContainerFactory() {
    ConcurrentKafkaListenerContainerFactory<String, Object> factory =
        new ConcurrentKafkaListenerContainerFactory<>();
    factory.setConsumerFactory(consumerFactory());
    factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL);
    return factory;
  }
}
</file>

<file path="data-ingest-service/src/main/java/com/platform/data/ingest/controller/IngestController.java">
package com.platform.data.ingest.controller;

import com.platform.data.ingest.dto.IngestBatchRequest;
import com.platform.data.ingest.service.DynamicIngestService;
import java.util.Map;
import java.util.concurrent.CompletableFuture;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

/** REST controller for data ingestion. */
@RestController
@RequestMapping("/api/ingest")
public class IngestController {

  private final DynamicIngestService ingestService;

  public IngestController(DynamicIngestService ingestService) {
    this.ingestService = ingestService;
  }

  /**
   * Ingests a single row for a specific tenant.
   *
   * @param tenantId The tenant identifier
   * @param payload The data to ingest
   * @return Success response
   */
  @PostMapping("/{tenantId}")
  public ResponseEntity<Map<String, String>> ingest(
      @PathVariable String tenantId, @RequestBody Map<String, Object> payload) {
    ingestService.ingest(tenantId, payload);
    return ResponseEntity.ok(Map.of("status", "success", "tenant", tenantId));
  }

  /**
   * Ingests a batch of rows.
   *
   * @param request Batch ingestion request
   * @return Success response
   */
  @PostMapping("/batch")
  public CompletableFuture<ResponseEntity<Map<String, Object>>> ingestBatch(
      @RequestBody IngestBatchRequest request) {
    return ingestService
        .ingestBatchAsync(request)
        .thenApply(
            v ->
                ResponseEntity.ok(
                    Map.of(
                        "status", "success",
                        "tenant", request.tenantId(),
                        "rows", request.data().size())));
  }
}
</file>

<file path="data-ingest-service/src/main/java/com/platform/data/ingest/dto/IngestBatchRequest.java">
package com.platform.data.ingest.dto;

import java.util.List;
import java.util.Map;

/** Envelope DTO for batch ingestion requests. Contains tenant metadata and data payload. */
public record IngestBatchRequest(
    String tenantId, String periodicity, List<Map<String, Object>> data) {
  /** Validates the request. */
  public void validate() {
    if (tenantId == null || tenantId.isBlank()) {
      throw new IllegalArgumentException("tenantId cannot be null or empty");
    }
    if (periodicity == null || periodicity.isBlank()) {
      throw new IllegalArgumentException("periodicity cannot be null or empty");
    }
    if (data == null || data.isEmpty()) {
      throw new IllegalArgumentException("data cannot be null or empty");
    }
  }
}
</file>

<file path="data-ingest-service/src/main/java/com/platform/data/ingest/kafka/KafkaIngestConsumer.java">
package com.platform.data.ingest.kafka;

import com.platform.data.ingest.dto.IngestBatchRequest;
import com.platform.data.ingest.service.DynamicIngestService;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.stereotype.Service;

/**
 * Kafka consumer for platform-ingest topic. Processes batch ingestion requests with manual
 * acknowledgment.
 */
@Service
public class KafkaIngestConsumer {

  private static final Logger log = LoggerFactory.getLogger(KafkaIngestConsumer.class);

  private final DynamicIngestService ingestService;

  public KafkaIngestConsumer(DynamicIngestService ingestService) {
    this.ingestService = ingestService;
  }

  /**
   * Consumes batch ingestion requests from Kafka. Only acknowledges after async processing
   * completes.
   *
   * @param request Batch ingestion request
   * @param ack Manual acknowledgment
   */
  @KafkaListener(
      topics = "platform-ingest",
      groupId = "data-ingest-group",
      containerFactory = "kafkaListenerContainerFactory")
  public void consume(IngestBatchRequest request, Acknowledgment ack) {
    log.info("Received batch for tenant: {}, rows: {}", request.tenantId(), request.data().size());

    ingestService
        .ingestBatchAsync(request)
        .thenRun(
            () -> {
              ack.acknowledge();
              log.info(
                  "Batch processed and acknowledged: tenant={}, rows={}",
                  request.tenantId(),
                  request.data().size());
            })
        .exceptionally(
            ex -> {
              log.error(
                  "Batch ingestion failed for tenant: {}. Message will be redelivered.",
                  request.tenantId(),
                  ex);
              // Don't acknowledge - message will be redelivered
              return null;
            });
  }
}
</file>

<file path="data-ingest-service/src/main/java/com/platform/data/ingest/IngestServiceApplication.java">
package com.platform.data.ingest;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

/** Main application class for the Data Ingest Service. */
@SpringBootApplication
public class IngestServiceApplication {

  public static void main(String[] args) {
    SpringApplication.run(IngestServiceApplication.class, args);
  }
}
</file>

<file path="data-ingest-service/src/test/java/com/platform/data/ingest/integration/IngestServiceIntegrationTest.java">
package com.platform.data.ingest.integration;

import static org.assertj.core.api.Assertions.assertThat;

import com.datastax.oss.driver.api.core.CqlSession;
import com.datastax.oss.driver.api.core.CqlSessionBuilder;
import com.platform.data.common.config.TenantConfig;
import com.platform.data.common.registry.TenantConfigRegistry;
import com.platform.data.common.test.SchemaInit;
import com.platform.data.ingest.dto.IngestBatchRequest;
import com.platform.data.ingest.service.DynamicIngestService;
import java.math.BigDecimal;
import java.net.InetSocketAddress;
import java.time.Instant;
import java.time.LocalDate;
import java.util.*;
import org.junit.jupiter.api.AfterEach;
import org.junit.jupiter.api.BeforeAll;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.testcontainers.containers.CassandraContainer;
import org.testcontainers.junit.jupiter.Container;
import org.testcontainers.junit.jupiter.Testcontainers;

/** Integration test for DynamicIngestService with real Cassandra. */
@Testcontainers
class IngestServiceIntegrationTest {

  private static final Logger log = LoggerFactory.getLogger(IngestServiceIntegrationTest.class);

  @Container
  static CassandraContainer<?> cassandra =
      new CassandraContainer<>("cassandra:4.1").withExposedPorts(9042);

  private static CqlSession session;
  private static DynamicIngestService ingestService;
  private static TenantConfigRegistry registry;

  @BeforeAll
  static void setup() {
    cassandra.start();

    session =
        new CqlSessionBuilder()
            .addContactPoint(
                new InetSocketAddress(cassandra.getHost(), cassandra.getMappedPort(9042)))
            .withLocalDatacenter("datacenter1")
            .build();

    log.info("Connected to Cassandra at {}:{}", cassandra.getHost(), cassandra.getMappedPort(9042));

    initializeSchema();

    registry = new TenantConfigRegistry();
    ingestService = new DynamicIngestService(session, registry);

    registerTenant();
  }

  @AfterEach
  void cleanup() {
    session.execute("TRUNCATE " + SchemaInit.getKeyspace() + "." + SchemaInit.getTableName());
    log.info("Cleaned up test data");
  }

  private static void initializeSchema() {
    log.info("Initializing schema...");
    session.execute(SchemaInit.getKeyspaceCreation());
    session.execute("USE " + SchemaInit.getKeyspace());

    String schema = SchemaInit.getSchema();
    String[] statements = schema.split(";");

    for (String statement : statements) {
      String trimmed = statement.trim();
      if (!trimmed.isEmpty()) {
        session.execute(trimmed);
      }
    }

    log.info("Schema initialized successfully");
  }

  private static void registerTenant() {
    TenantConfig config =
        TenantConfig.withBucket(
            SchemaInit.getKeyspace(),
            SchemaInit.getTableName(),
            List.of("tenant_id", "instrument_id", "period_year"),
            "period_year",
            Set.of("data"));

    registry.register("IBM", "DAILY", "NUMERIC", config);
    log.info("Registered tenant: IBM");
  }

  @Test
  void testBatchIngestion_persistsDataCorrectly() {
    log.info("=== Testing Batch Ingestion ===");

    // Create batch with 5 rows
    List<Map<String, Object>> batch = new ArrayList<>();
    for (int i = 0; i < 5; i++) {
      batch.add(createRow("IBM", "IBM_STOCK", LocalDate.of(2024, 1, 10 + i), "revenue", 100.0 + i));
    }

    IngestBatchRequest request = new IngestBatchRequest("IBM", "DAILY", batch);

    // Ingest
    ingestService.ingestBatchAsync(request).join();

    // Verify data persisted
    var results =
        session
            .execute(
                "SELECT * FROM "
                    + SchemaInit.getKeyspace()
                    + "."
                    + SchemaInit.getTableName()
                    + " WHERE tenant_id = 'IBM' AND instrument_id = 'IBM_STOCK' AND period_year ="
                    + " 2024")
            .all();

    assertThat(results).hasSize(5);
    log.info("Successfully verified 5 rows persisted");
  }

  @Test
  void testPartitionLevelBatching_groupsByYear() {
    log.info("=== Testing Partition-Level Batching ===");

    // Create batch with mixed years
    List<Map<String, Object>> batch = new ArrayList<>();

    // 3 rows for 2023
    for (int i = 0; i < 3; i++) {
      batch.add(
          createRow("IBM", "IBM_STOCK", LocalDate.of(2023, 12, 10 + i), "revenue", 100.0 + i));
    }

    // 3 rows for 2024
    for (int i = 0; i < 3; i++) {
      batch.add(createRow("IBM", "IBM_STOCK", LocalDate.of(2024, 1, 10 + i), "profit", 50.0 + i));
    }

    IngestBatchRequest request = new IngestBatchRequest("IBM", "DAILY", batch);

    // Ingest
    ingestService.ingestBatchAsync(request).join();

    // Verify 2023 data
    var results2023 =
        session
            .execute(
                "SELECT * FROM "
                    + SchemaInit.getKeyspace()
                    + "."
                    + SchemaInit.getTableName()
                    + " WHERE tenant_id = 'IBM' AND instrument_id = 'IBM_STOCK' AND period_year ="
                    + " 2023")
            .all();

    // Verify 2024 data
    var results2024 =
        session
            .execute(
                "SELECT * FROM "
                    + SchemaInit.getKeyspace()
                    + "."
                    + SchemaInit.getTableName()
                    + " WHERE tenant_id = 'IBM' AND instrument_id = 'IBM_STOCK' AND period_year ="
                    + " 2024")
            .all();

    assertThat(results2023).hasSize(3);
    assertThat(results2024).hasSize(3);
    log.info("Successfully verified partition-level batching: 3 rows in 2023, 3 rows in 2024");
  }

  private Map<String, Object> createRow(
      String tenantId, String instrumentId, LocalDate periodDate, String fieldId, double value) {
    Map<String, Object> row = new HashMap<>();
    row.put("tenant_id", tenantId);
    row.put("instrument_id", instrumentId);
    row.put("period_date", periodDate);
    row.put("field_id", fieldId);

    Map<String, Object> dataPoint = new HashMap<>();
    dataPoint.put("value", BigDecimal.valueOf(value));
    dataPoint.put("report_time", Instant.now());
    row.put("data", dataPoint);

    return row;
  }
}
</file>

<file path="data-query-service/src/main/java/com/platform/data/query/config/CassandraConfig.java">
package com.platform.data.query.config;

import com.datastax.oss.driver.api.core.CqlSession;
import com.datastax.oss.driver.api.core.CqlSessionBuilder;
import java.net.InetSocketAddress;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

/** Cassandra configuration for the query service. */
@Configuration
public class CassandraConfig {

  @Value("${cassandra.contact-points}")
  private String contactPoints;

  @Value("${cassandra.port}")
  private int port;

  @Value("${cassandra.local-datacenter}")
  private String localDatacenter;

  @Value("${cassandra.keyspace}")
  private String keyspace;

  @Bean
  public CqlSession cqlSession() {
    return new CqlSessionBuilder()
        .addContactPoint(new InetSocketAddress(contactPoints, port))
        .withLocalDatacenter(localDatacenter)
        .withKeyspace(keyspace)
        .build();
  }
}
</file>

<file path="data-query-service/src/main/java/com/platform/data/query/controller/QueryController.java">
package com.platform.data.query.controller;

import com.platform.data.query.service.DynamicRetrievalService;
import java.util.List;
import java.util.Map;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

/** REST controller for data querying. */
@RestController
@RequestMapping("/api/query")
public class QueryController {

  private final DynamicRetrievalService retrievalService;

  public QueryController(DynamicRetrievalService retrievalService) {
    this.retrievalService = retrievalService;
  }

  /**
   * Queries data for a specific tenant.
   *
   * @param tenantId The tenant identifier
   * @param criteria Query criteria (must include start_date and end_date)
   * @return List of matching records
   */
  @PostMapping("/{tenantId}")
  public ResponseEntity<List<Map<String, Object>>> query(
      @PathVariable String tenantId, @RequestBody Map<String, Object> criteria) {
    List<Map<String, Object>> results = retrievalService.retrieve(tenantId, criteria);
    return ResponseEntity.ok(results);
  }
}
</file>

<file path="data-query-service/src/main/java/com/platform/data/query/QueryServiceApplication.java">
package com.platform.data.query;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

/** Main application class for the Data Query Service. */
@SpringBootApplication
public class QueryServiceApplication {

  public static void main(String[] args) {
    SpringApplication.run(QueryServiceApplication.class, args);
  }
}
</file>

<file path="README.md">
# Data Platform Core

A generic, multi-tenant Data Platform built with Java 21, Spring Boot 3.x, and Datastax Cassandra Driver 4.x.

## üéØ Key Features

- **Dynamic Schema**: No static `@Table` classes - schema is configured at runtime
- **Auto-Partitioning**: Automatic bucket column injection (e.g., extracting year from dates)
- **Scatter-Gather Queries**: Parallel async queries across partition buckets
- **UDT Support**: Automatic conversion between Map ‚Üî UdtValue
- **Clean JSON**: All responses are JSON-serializable (no Cassandra types exposed)

## üì¶ Project Structure

```
data-platform-core/
‚îú‚îÄ‚îÄ data-common/              # Shared utilities and configuration models
‚îÇ   ‚îú‚îÄ‚îÄ TenantConfig.java     # Runtime metadata (keyspace, table, partition keys, UDTs)
‚îÇ   ‚îî‚îÄ‚îÄ SchemaInit.java       # Test helper with CQL schema
‚îú‚îÄ‚îÄ data-ingest-service/      # Write path (REST API)
‚îÇ   ‚îî‚îÄ‚îÄ DynamicIngestService  # Handles ingestion with auto-partitioning & UDT conversion
‚îî‚îÄ‚îÄ data-query-service/       # Read path (REST API)
    ‚îî‚îÄ‚îÄ DynamicRetrievalService # Scatter-gather queries with parallel execution
```

## üõ†Ô∏è Tech Stack

- **Java 21** with Records and Text Blocks
- **Spring Boot 3.2.1** (Web only, no Spring Data)
- **Datastax Java Driver 4.17.0** (Raw CqlSession + QueryBuilder)
- **JUnit 5** + **Testcontainers** for integration testing
- **Maven** multi-module project

## üöÄ Quick Start

### Build the Project

```bash
cd data-platform-core
mvn clean install
```

### Run the Integration Test

The integration test proves the entire system works end-to-end:

```bash
cd data-query-service
mvn test -Dtest=GenericPlatformIntegrationTest
```

This test:
1. Starts Cassandra via Testcontainers
2. Creates the schema (UDT + bucketed table)
3. Ingests data for IBM across Dec 2023 and Jan 2024 (two different partitions)
4. Queries data from 2023-12-01 to 2024-02-01
5. Verifies scatter-gather worked and returned data from both years

### Run the Services

**Ingest Service** (port 8081):
```bash
cd data-ingest-service
mvn spring-boot:run
```

**Query Service** (port 8082):
```bash
cd data-query-service
mvn spring-boot:run
```

## üìä Example Schema

The test schema demonstrates a typical time-series use case:

```sql
CREATE TYPE IF NOT EXISTS numeric_data_point (
    value decimal,
    report_time timestamp
);

CREATE TABLE IF NOT EXISTS DailyNumeric (
    tenant_id text,
    instrument_id text,
    period_year int,           -- Bucket column
    period_date date,
    field_id text,
    data frozen<numeric_data_point>,  -- UDT column
    PRIMARY KEY ((tenant_id, instrument_id, period_year), period_date, field_id)
);
```

## üîß How It Works

### 1. Tenant Configuration

```java
TenantConfig config = TenantConfig.withBucket(
    "test_keyspace",
    "DailyNumeric",
    List.of("tenant_id", "instrument_id", "period_year"),
    "period_year",  // Bucket column
    Set.of("data")  // UDT columns
);
```

### 2. Ingestion

```java
Map<String, Object> payload = Map.of(
    "tenant_id", "IBM",
    "instrument_id", "IBM_STOCK",
    "period_date", LocalDate.of(2024, 1, 10),
    "field_id", "revenue",
    "data", Map.of(
        "value", BigDecimal.valueOf(102.7),
        "report_time", Instant.now()
    )
);

ingestService.ingest("IBM", payload);
```

The service will:
- Extract the year (2024) from `period_date`
- Inject `period_year: 2024` into the payload
- Convert the `data` Map to a `UdtValue`
- Build and execute the CQL insert

### 3. Querying

```java
Map<String, Object> criteria = Map.of(
    "tenant_id", "IBM",
    "instrument_id", "IBM_STOCK",
    "start_date", "2023-12-01",
    "end_date", "2024-02-01"
);

List<Map<String, Object>> results = retrievalService.retrieve("IBM", criteria);
```

The service will:
- Calculate year range: 2023-2024
- Fire 2 parallel async queries (one per year bucket)
- Gather and merge results
- Convert all `UdtValue` back to `Map` for clean JSON

## üß™ Testing

The `GenericPlatformIntegrationTest` is the proof that everything works:

```bash
mvn test -Dtest=GenericPlatformIntegrationTest
```

Expected output:
```
=== Integration Test PASSED ===
Successfully demonstrated:
  ‚úì Multi-partition ingestion (2023 & 2024)
  ‚úì Scatter-gather query across year buckets
  ‚úì UDT conversion (Map -> UdtValue -> Map)
  ‚úì Clean JSON-serializable results
```

## üìù API Endpoints

### Ingest Service (8081)

**POST** `/api/ingest/{tenantId}`
```json
{
  "tenant_id": "IBM",
  "instrument_id": "IBM_STOCK",
  "period_date": "2024-01-10",
  "field_id": "revenue",
  "data": {
    "value": 102.7,
    "report_time": "2024-01-10T12:00:00Z"
  }
}
```

### Query Service (8082)

**POST** `/api/query/{tenantId}`
```json
{
  "tenant_id": "IBM",
  "instrument_id": "IBM_STOCK",
  "start_date": "2023-12-01",
  "end_date": "2024-02-01"
}
```

## üèóÔ∏è Architecture Highlights

### No Static @Table Classes
Traditional Spring Data Cassandra requires:
```java
@Table
class MyEntity { ... }
```

This platform uses **runtime configuration** instead, allowing:
- Different tenants to use different schemas
- Schema evolution without code changes
- True multi-tenancy

### Scatter-Gather Pattern
For bucketed tables, queries automatically:
1. Calculate bucket range (e.g., years 2023-2024)
2. Fire parallel `executeAsync()` calls
3. Gather results via `CompletableFuture.join()`
4. Merge and return

### UDT Handling
- **Ingestion**: `Map<String, Object>` ‚Üí `UdtValue` (via session metadata)
- **Retrieval**: `UdtValue` ‚Üí `Map<String, Object>` (recursive conversion)
- **Result**: Clean JSON with no Cassandra types

## üìö Further Reading

- [Datastax Java Driver Docs](https://docs.datastax.com/en/developer/java-driver/4.17/)
- [QueryBuilder API](https://docs.datastax.com/en/developer/java-driver/4.17/manual/query_builder/)
- [Testcontainers Cassandra](https://www.testcontainers.org/modules/databases/cassandra/)

## üìÑ License

MIT License - feel free to use this as a template for your own data platforms!
</file>

<file path="data-common/src/main/java/com/platform/data/common/mapper/UdtMapper.java">
package com.platform.data.common.mapper;

import com.datastax.oss.driver.api.core.CqlSession;
import com.datastax.oss.driver.api.core.data.UdtValue;
import com.datastax.oss.driver.api.core.type.UserDefinedType;
import java.math.BigDecimal;
import java.time.Instant;
import java.util.HashMap;
import java.util.Map;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Centralized UDT conversion utility. Ensures consistent UDT ‚Üî Map conversion across ingest and
 * query services.
 */
public class UdtMapper {

  private static final Logger log = LoggerFactory.getLogger(UdtMapper.class);

  /**
   * Converts a Map to a UdtValue. Handles BigDecimal, Instant, String parsing, and nested UDTs.
   *
   * @param session CQL session for metadata access
   * @param keyspace Keyspace name
   * @param udtName UDT type name
   * @param data Map containing field values
   * @return UdtValue instance
   */
  public static UdtValue toUdt(
      CqlSession session, String keyspace, String udtName, Map<String, Object> data) {
    // Get the UDT metadata from the session
    UserDefinedType udtType =
        session
            .getMetadata()
            .getKeyspace(keyspace)
            .orElseThrow(() -> new IllegalStateException("Keyspace not found: " + keyspace))
            .getUserDefinedType(udtName)
            .orElseThrow(() -> new IllegalStateException("UDT not found: " + udtName));

    UdtValue udtValue = udtType.newValue();

    // Populate the UDT fields
    for (Map.Entry<String, Object> field : data.entrySet()) {
      String fieldName = field.getKey();
      Object fieldValue = field.getValue();

      if (fieldValue == null) {
        // Skip null values
        continue;
      }

      udtValue = setUdtField(session, keyspace, udtValue, fieldName, fieldValue);
    }

    return udtValue;
  }

  /** Sets a single field in a UdtValue, handling type conversion. */
  private static UdtValue setUdtField(
      CqlSession session, String keyspace, UdtValue udtValue, String fieldName, Object fieldValue) {
    if (fieldValue instanceof BigDecimal bd) {
      return udtValue.setBigDecimal(fieldName, bd);
    } else if (fieldValue instanceof Double d) {
      return udtValue.setBigDecimal(fieldName, BigDecimal.valueOf(d));
    } else if (fieldValue instanceof Integer i) {
      return udtValue.setBigDecimal(fieldName, BigDecimal.valueOf(i));
    } else if (fieldValue instanceof Long l) {
      return udtValue.setBigDecimal(fieldName, BigDecimal.valueOf(l));
    } else if (fieldValue instanceof Instant instant) {
      return udtValue.setInstant(fieldName, instant);
    } else if (fieldValue instanceof String str) {
      // Try to parse as timestamp if field name contains "time"
      if (fieldName.toLowerCase().contains("time")) {
        try {
          return udtValue.setInstant(fieldName, Instant.parse(str));
        } catch (Exception e) {
          log.warn("Failed to parse timestamp for field {}: {}", fieldName, str);
          return udtValue.setString(fieldName, str);
        }
      } else {
        return udtValue.setString(fieldName, str);
      }
    } else if (fieldValue instanceof Map) {
      // Nested UDT - recursive conversion
      @SuppressWarnings("unchecked")
      Map<String, Object> nestedMap = (Map<String, Object>) fieldValue;
      UdtValue nestedUdt = toUdt(session, keyspace, fieldName, nestedMap);
      return udtValue.setUdtValue(fieldName, nestedUdt);
    } else {
      log.warn("Unsupported UDT field type: {} for field: {}", fieldValue.getClass(), fieldName);
      return udtValue;
    }
  }

  /**
   * Converts a UdtValue to a Map. Recursively handles nested UDTs for clean JSON serialization.
   *
   * @param udtValue UDT to convert
   * @return Map representation
   */
  public static Map<String, Object> toMap(UdtValue udtValue) {
    if (udtValue == null) {
      return new HashMap<>();
    }

    Map<String, Object> map = new HashMap<>();

    for (int i = 0; i < udtValue.getType().getFieldNames().size(); i++) {
      String fieldName = udtValue.getType().getFieldNames().get(i).asInternal();
      Object fieldValue = udtValue.getObject(i);

      if (fieldValue == null) {
        map.put(fieldName, null);
      } else if (fieldValue instanceof UdtValue nestedUdt) {
        // Recursive conversion for nested UDTs
        map.put(fieldName, toMap(nestedUdt));
      } else {
        map.put(fieldName, fieldValue);
      }
    }

    return map;
  }
}
</file>

<file path="data-common/src/main/java/com/platform/data/common/util/PartitionCalculator.java">
package com.platform.data.common.util;

import com.platform.data.common.config.TenantConfig;
import java.time.Instant;
import java.time.LocalDate;
import java.time.ZoneId;
import java.util.Map;

/**
 * Centralized partition bucket calculation utility. Ensures consistent partitioning logic across
 * ingest and query services.
 */
public class PartitionCalculator {

  /**
   * Calculates the bucket value for a given payload.
   *
   * @param config Tenant configuration
   * @param payload Data payload
   * @return Bucket value (e.g., year as Integer), or null if no bucket configured
   */
  public static Object calculateBucket(TenantConfig config, Map<String, Object> payload) {
    if (!config.hasBucket()) {
      return null;
    }

    String bucketColumn = config.getBucketColumnOrThrow();

    // For year-based bucketing, look for a date field
    // Common date field names: period_date, date, timestamp, etc.
    Object dateValue = findDateValue(payload);

    if (dateValue == null) {
      // Return null instead of throwing exception for missing date
      return null;
    }

    return extractYear(dateValue);
  }

  /** Finds a date value in the payload. Looks for common date field names. */
  private static Object findDateValue(Map<String, Object> payload) {
    // Try common field names
    String[] dateFields = {"period_date", "date", "timestamp", "report_date", "event_date"};

    for (String field : dateFields) {
      if (payload.containsKey(field)) {
        return payload.get(field);
      }
    }

    return null;
  }

  /** Extracts the year from a date value. */
  private static int extractYear(Object dateValue) {
    if (dateValue instanceof LocalDate localDate) {
      return localDate.getYear();
    } else if (dateValue instanceof String dateStr) {
      LocalDate date = LocalDate.parse(dateStr);
      return date.getYear();
    } else if (dateValue instanceof Long timestamp) {
      return Instant.ofEpochMilli(timestamp).atZone(ZoneId.systemDefault()).getYear();
    } else if (dateValue instanceof Instant instant) {
      return instant.atZone(ZoneId.systemDefault()).getYear();
    }

    throw new IllegalArgumentException(
        "Unsupported date type for bucket calculation: " + dateValue.getClass());
  }

  /**
   * Calculates the year range for a date range query. Used by query service for scatter-gather.
   *
   * @param startDate Start date
   * @param endDate End date
   * @return Array of all years in the range [startYear, startYear+1, ..., endYear]
   */
  public static int[] calculateYearRange(LocalDate startDate, LocalDate endDate) {
    int startYear = startDate.getYear();
    int endYear = endDate.getYear();

    int[] years = new int[endYear - startYear + 1];
    for (int i = 0; i < years.length; i++) {
      years[i] = startYear + i;
    }

    return years;
  }
}
</file>

<file path="data-common/src/test/java/com/platform/data/common/util/PartitionCalculatorTest.java">
package com.platform.data.common.util;

import static org.assertj.core.api.Assertions.*;

import com.platform.data.common.config.TenantConfig;
import java.time.LocalDate;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Set;
import org.junit.jupiter.api.Test;

/** Unit tests for PartitionCalculator. */
class PartitionCalculatorTest {

  @Test
  void testCalculateBucket_extractsYearFromDate() {
    // Arrange
    TenantConfig config =
        TenantConfig.withBucket(
            "test_ks", "test_table", List.of("tenant_id", "period_year"), "period_year", Set.of());

    Map<String, Object> payload = new HashMap<>();
    payload.put("period_date", LocalDate.of(2024, 6, 15));

    // Act
    Object bucket = PartitionCalculator.calculateBucket(config, payload);

    // Assert
    assertThat(bucket).isEqualTo(2024);
  }

  @Test
  void testCalculateBucket_withNoBucketColumn_returnsNull() {
    // Arrange
    TenantConfig config =
        TenantConfig.withoutBucket("test_ks", "test_table", List.of("tenant_id"), Set.of());

    Map<String, Object> payload = new HashMap<>();
    payload.put("period_date", LocalDate.of(2024, 6, 15));

    // Act
    Object bucket = PartitionCalculator.calculateBucket(config, payload);

    // Assert
    assertThat(bucket).isNull();
  }

  @Test
  void testCalculateBucket_withMissingDateField_returnsNull() {
    // Arrange
    TenantConfig config =
        TenantConfig.withBucket(
            "test_ks", "test_table", List.of("tenant_id", "period_year"), "period_year", Set.of());

    Map<String, Object> payload = new HashMap<>();
    // No period_date field

    // Act
    Object bucket = PartitionCalculator.calculateBucket(config, payload);

    // Assert
    assertThat(bucket).isNull();
  }

  @Test
  void testCalculateYearRange_singleYear() {
    // Arrange
    LocalDate startDate = LocalDate.of(2024, 1, 1);
    LocalDate endDate = LocalDate.of(2024, 12, 31);

    // Act
    int[] years = PartitionCalculator.calculateYearRange(startDate, endDate);

    // Assert
    assertThat(years).containsExactly(2024);
  }

  @Test
  void testCalculateYearRange_multipleYears() {
    // Arrange
    LocalDate startDate = LocalDate.of(2023, 6, 1);
    LocalDate endDate = LocalDate.of(2025, 3, 31);

    // Act
    int[] years = PartitionCalculator.calculateYearRange(startDate, endDate);

    // Assert
    assertThat(years).containsExactly(2023, 2024, 2025);
  }

  @Test
  void testCalculateYearRange_acrossYearBoundary() {
    // Arrange
    LocalDate startDate = LocalDate.of(2023, 12, 15);
    LocalDate endDate = LocalDate.of(2024, 1, 15);

    // Act
    int[] years = PartitionCalculator.calculateYearRange(startDate, endDate);

    // Assert
    assertThat(years).containsExactly(2023, 2024);
  }
}
</file>

<file path="data-ingest-service/src/main/java/com/platform/data/ingest/service/DynamicIngestService.java">
package com.platform.data.ingest.service;

import com.datastax.oss.driver.api.core.CqlSession;
import com.datastax.oss.driver.api.core.cql.*;
import com.datastax.oss.driver.api.core.data.UdtValue;
import com.datastax.oss.driver.api.querybuilder.QueryBuilder;
import com.platform.data.common.config.TenantConfig;
import com.platform.data.common.mapper.UdtMapper;
import com.platform.data.common.model.PartitionKey;
import com.platform.data.common.registry.TenantConfigRegistry;
import com.platform.data.common.util.PartitionCalculator;
import com.platform.data.ingest.dto.IngestBatchRequest;
import java.math.BigDecimal;
import java.time.LocalDate;
import java.util.*;
import java.util.concurrent.CompletableFuture;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.stereotype.Service;

/**
 * Dynamic ingestion service with smart batching support. Features: - Partition-level batching for
 * high throughput - Automatic UDT conversion using shared mapper - Polymorphic routing via
 * TenantConfigRegistry
 */
@Service
public class DynamicIngestService {

  private static final Logger log = LoggerFactory.getLogger(DynamicIngestService.class);

  private final CqlSession session;
  private final TenantConfigRegistry registry;

  public DynamicIngestService(CqlSession session, TenantConfigRegistry registry) {
    this.session = session;
    this.registry = registry;
  }

  /** Ingests a single row (legacy method for backward compatibility). */
  public void ingest(String tenantId, Map<String, Object> payload) {
    // For single row, create a batch request with one item
    IngestBatchRequest request =
        new IngestBatchRequest(
            tenantId,
            "DAILY", // Default periodicity
            List.of(payload));

    ingestBatchAsync(request).join();
  }

  /**
   * Ingests a batch of rows with partition-level batching.
   *
   * @param request Batch request containing tenant metadata and data
   * @return CompletableFuture that completes when all batches are written
   */
  public CompletableFuture<Void> ingestBatchAsync(IngestBatchRequest request) {
    request.validate();

    log.info(
        "Processing batch for tenant: {}, periodicity: {}, rows: {}",
        request.tenantId(),
        request.periodicity(),
        request.data().size());

    // Step 1: Infer data type from first row
    String dataType = inferDataType(request.data().get(0));
    log.debug("Inferred data type: {}", dataType);

    // Step 2: Lookup configuration
    TenantConfig config = registry.lookup(request.tenantId(), request.periodicity(), dataType);

    // Step 3: Group rows by partition
    Map<PartitionKey, List<BoundStatement>> groups = groupByPartition(config, request.data());
    log.info("Grouped {} rows into {} partition batches", request.data().size(), groups.size());

    // Step 4: Execute batches asynchronously
    List<CompletableFuture<AsyncResultSet>> futures = new ArrayList<>();

    for (Map.Entry<PartitionKey, List<BoundStatement>> entry : groups.entrySet()) {
      // Build batch by adding statements one by one
      var batchBuilder = BatchStatement.builder(DefaultBatchType.LOGGED);
      for (BoundStatement stmt : entry.getValue()) {
        batchBuilder.addStatement(stmt);
      }
      BatchStatement batch = batchBuilder.build();

      CompletableFuture<AsyncResultSet> future = session.executeAsync(batch).toCompletableFuture();
      futures.add(future);

      log.debug(
          "Submitted batch for partition: {} with {} statements",
          entry.getKey(),
          entry.getValue().size());
    }

    // Step 5: Wait for all batches to complete
    return CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
        .thenRun(() -> log.info("Batch ingestion complete for tenant: {}", request.tenantId()));
  }

  /** Infers the data type from the first row. */
  private String inferDataType(Map<String, Object> firstRow) {
    // Look for a "data" field (common UDT field name)
    Object dataField = firstRow.get("data");

    if (dataField instanceof Map) {
      @SuppressWarnings("unchecked")
      Map<String, Object> dataMap = (Map<String, Object>) dataField;
      Object value = dataMap.get("value");

      if (value instanceof Number) {
        return "NUMERIC";
      } else if (value instanceof String) {
        return "STRING";
      }
    }

    // Default to NUMERIC
    return "NUMERIC";
  }

  /** Groups rows by partition key. */
  private Map<PartitionKey, List<BoundStatement>> groupByPartition(
      TenantConfig config, List<Map<String, Object>> data) {
    Map<PartitionKey, List<BoundStatement>> groups = new HashMap<>();

    for (Map<String, Object> row : data) {
      // Create a mutable copy
      Map<String, Object> enrichedRow = new HashMap<>(row);

      // Calculate and inject bucket if configured
      Object bucket = PartitionCalculator.calculateBucket(config, enrichedRow);
      if (bucket != null) {
        enrichedRow.put(config.getBucketColumnOrThrow(), bucket);
      }

      // Process UDTs using shared mapper
      for (String udtColumn : config.udtColumns()) {
        if (enrichedRow.containsKey(udtColumn) && enrichedRow.get(udtColumn) instanceof Map) {
          @SuppressWarnings("unchecked")
          Map<String, Object> udtMap = (Map<String, Object>) enrichedRow.get(udtColumn);
          // Use the UDT type name, not the column name
          // For now, hardcode the UDT type name (in production, this would come from
          // config)
          String udtTypeName = "numeric_data_point";
          UdtValue udtValue = UdtMapper.toUdt(session, config.keyspace(), udtTypeName, udtMap);
          enrichedRow.put(udtColumn, udtValue);
        }
      }

      // Extract partition key values
      List<Object> pkValues = config.partitionKeys().stream().map(enrichedRow::get).toList();
      PartitionKey pk = new PartitionKey(pkValues);

      // Build insert statement
      BoundStatement stmt = buildInsertStatement(config, enrichedRow);

      // Group by partition
      groups.computeIfAbsent(pk, k -> new ArrayList<>()).add(stmt);
    }

    return groups;
  }

  /** Builds an insert statement for a single row. */
  private BoundStatement buildInsertStatement(TenantConfig config, Map<String, Object> payload) {
    // Build the insert query
    var insertInto = QueryBuilder.insertInto(config.keyspace(), config.tableName());

    // Start with the first column
    var iterator = payload.entrySet().iterator();
    if (!iterator.hasNext()) {
      throw new IllegalArgumentException("Payload cannot be empty");
    }

    var firstEntry = iterator.next();
    var regularInsert =
        insertInto.value(firstEntry.getKey(), QueryBuilder.bindMarker(firstEntry.getKey()));

    // Chain the rest of the values
    while (iterator.hasNext()) {
      var entry = iterator.next();
      regularInsert = regularInsert.value(entry.getKey(), QueryBuilder.bindMarker(entry.getKey()));
    }

    // Prepare the statement
    PreparedStatement prepared = session.prepare(regularInsert.build());

    // Bind the values
    BoundStatement bound = prepared.boundStatementBuilder().build();

    for (Map.Entry<String, Object> entry : payload.entrySet()) {
      String key = entry.getKey();
      Object value = entry.getValue();

      if (value instanceof String s) {
        bound = bound.setString(key, s);
      } else if (value instanceof Integer i) {
        bound = bound.setInt(key, i);
      } else if (value instanceof LocalDate ld) {
        bound = bound.setLocalDate(key, ld);
      } else if (value instanceof UdtValue udt) {
        bound = bound.setUdtValue(key, udt);
      } else if (value instanceof BigDecimal bd) {
        bound = bound.setBigDecimal(key, bd);
      } else {
        log.warn("Unsupported type for column {}: {}", key, value.getClass());
      }
    }

    return bound;
  }
}
</file>

<file path="data-query-service/src/main/java/com/platform/data/query/service/DynamicRetrievalService.java">
package com.platform.data.query.service;

import com.datastax.oss.driver.api.core.CqlSession;
import com.datastax.oss.driver.api.core.cql.*;
import com.datastax.oss.driver.api.core.data.UdtValue;
import com.datastax.oss.driver.api.querybuilder.QueryBuilder;
import com.datastax.oss.driver.api.querybuilder.select.Select;
import com.platform.data.common.config.TenantConfig;
import com.platform.data.common.mapper.UdtMapper;
import com.platform.data.common.registry.TenantConfigRegistry;
import java.time.LocalDate;
import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.stream.Collectors;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.stereotype.Service;

/**
 * Dynamic retrieval service implementing scatter-gather pattern for bucketed queries. Features: -
 * Parallel async queries across year buckets - Automatic UDT to Map conversion using shared mapper
 * - Polymorphic routing via TenantConfigRegistry
 */
@Service
public class DynamicRetrievalService {

  private static final Logger log = LoggerFactory.getLogger(DynamicRetrievalService.class);

  private final CqlSession session;
  private final TenantConfigRegistry registry;

  public DynamicRetrievalService(CqlSession session, TenantConfigRegistry registry) {
    this.session = session;
    this.registry = registry;
  }

  /**
   * Retrieves data for a specific tenant based on criteria. Implements scatter-gather pattern for
   * bucketed tables.
   *
   * @param tenantId The tenant identifier
   * @param criteria Query criteria (must include start_date and end_date)
   * @return List of results as Map<String, Object>
   */
  public List<Map<String, Object>> retrieve(String tenantId, Map<String, Object> criteria) {
    // Extract date range
    LocalDate startDate = extractDate(criteria, "start_date");
    LocalDate endDate = extractDate(criteria, "end_date");

    if (startDate == null || endDate == null) {
      throw new IllegalArgumentException("start_date and end_date are required");
    }

    // Infer data type (default to NUMERIC for now)
    String dataType = "NUMERIC";
    String periodicity = "DAILY";

    // Lookup configuration
    TenantConfig config = registry.lookup(tenantId, periodicity, dataType);

    // Scatter-gather based on bucket configuration
    if (config.hasBucket()) {
      return scatterGatherQuery(config, criteria, startDate, endDate);
    } else {
      return singleQuery(config, criteria, startDate, endDate);
    }
  }

  /** Executes parallel queries across year buckets and gathers results. */
  private List<Map<String, Object>> scatterGatherQuery(
      TenantConfig config, Map<String, Object> criteria, LocalDate startDate, LocalDate endDate) {
    int startYear = startDate.getYear();
    int endYear = endDate.getYear();

    log.info(
        "Scatter-gather query from {} to {} ({} buckets)",
        startYear,
        endYear,
        (endYear - startYear + 1));

    // Create async queries for each year bucket
    List<CompletableFuture<List<Row>>> futures = new ArrayList<>();

    for (int year = startYear; year <= endYear; year++) {
      Map<String, Object> bucketCriteria = new HashMap<>(criteria);
      bucketCriteria.put(config.getBucketColumnOrThrow(), year);

      CompletableFuture<List<Row>> future =
          executeAsyncQuery(config, bucketCriteria, startDate, endDate);
      futures.add(future);
    }

    // Gather all results
    List<Row> allRows =
        futures.stream()
            .map(CompletableFuture::join)
            .flatMap(List::stream)
            .collect(Collectors.toList());

    log.info("Gathered {} total rows from {} buckets", allRows.size(), futures.size());

    // Convert rows to maps using shared mapper
    return allRows.stream().map(row -> convertRowToMap(row, config)).collect(Collectors.toList());
  }

  /** Executes a single query (no bucketing). */
  private List<Map<String, Object>> singleQuery(
      TenantConfig config, Map<String, Object> criteria, LocalDate startDate, LocalDate endDate) {
    log.info("Single query (no bucketing)");

    List<Row> rows = executeAsyncQuery(config, criteria, startDate, endDate).join();

    return rows.stream().map(row -> convertRowToMap(row, config)).collect(Collectors.toList());
  }

  /** Executes an async query and returns a future of rows. */
  private CompletableFuture<List<Row>> executeAsyncQuery(
      TenantConfig config, Map<String, Object> criteria, LocalDate startDate, LocalDate endDate) {
    // Build the select query
    Select select = QueryBuilder.selectFrom(config.keyspace(), config.tableName()).all();

    // Add WHERE clauses for partition keys
    for (String partitionKey : config.partitionKeys()) {
      Object value = criteria.get(partitionKey);
      if (value != null) {
        select = select.whereColumn(partitionKey).isEqualTo(QueryBuilder.bindMarker(partitionKey));
      }
    }

    // Note: bucket column is already part of partition keys, so no need to add it
    // again

    // Add date range filter
    select =
        select
            .whereColumn("period_date")
            .isGreaterThanOrEqualTo(QueryBuilder.bindMarker("start_date"))
            .whereColumn("period_date")
            .isLessThanOrEqualTo(QueryBuilder.bindMarker("end_date"));

    // Prepare and bind
    PreparedStatement prepared = session.prepare(select.build());
    BoundStatementBuilder builder = prepared.boundStatementBuilder();

    // Bind partition keys
    for (String partitionKey : config.partitionKeys()) {
      Object value = criteria.get(partitionKey);
      if (value != null) {
        if (value instanceof String s) {
          builder.setString(partitionKey, s);
        } else if (value instanceof Integer i) {
          builder.setInt(partitionKey, i);
        }
      }
    }

    // Note: bucket column binding is already handled by partition keys loop above

    // Bind date range
    builder.setLocalDate("start_date", startDate);
    builder.setLocalDate("end_date", endDate);

    BoundStatement bound = builder.build();

    // Execute async
    return session
        .executeAsync(bound)
        .toCompletableFuture()
        .thenApply(
            rs -> {
              List<Row> rows = new ArrayList<>();
              for (Row row : rs.currentPage()) {
                rows.add(row);
              }
              return rows;
            });
  }

  /** Converts a Cassandra Row to a Map, recursively converting UDTs using shared mapper. */
  private Map<String, Object> convertRowToMap(Row row, TenantConfig config) {
    Map<String, Object> result = new HashMap<>();

    for (int i = 0; i < row.getColumnDefinitions().size(); i++) {
      String columnName = row.getColumnDefinitions().get(i).getName().asInternal();
      Object value = row.getObject(i);

      if (value == null) {
        result.put(columnName, null);
      } else if (value instanceof UdtValue udtValue) {
        // Use shared UdtMapper for conversion
        result.put(columnName, UdtMapper.toMap(udtValue));
      } else {
        result.put(columnName, value);
      }
    }

    return result;
  }

  /** Extracts a LocalDate from criteria. */
  private LocalDate extractDate(Map<String, Object> criteria, String key) {
    Object value = criteria.get(key);
    if (value instanceof LocalDate ld) {
      return ld;
    } else if (value instanceof String str) {
      return LocalDate.parse(str);
    }
    return null;
  }
}
</file>

<file path="data-query-service/src/test/java/com/platform/data/query/GenericPlatformIntegrationTest.java">
package com.platform.data.query;

import static org.assertj.core.api.Assertions.assertThat;

import com.datastax.oss.driver.api.core.CqlSession;
import com.datastax.oss.driver.api.core.CqlSessionBuilder;
import com.platform.data.common.config.TenantConfig;
import com.platform.data.common.registry.TenantConfigRegistry;
import com.platform.data.common.test.SchemaInit;
import com.platform.data.ingest.dto.IngestBatchRequest;
import com.platform.data.ingest.service.DynamicIngestService;
import com.platform.data.query.service.DynamicRetrievalService;
import java.math.BigDecimal;
import java.net.InetSocketAddress;
import java.time.Instant;
import java.time.LocalDate;
import java.util.*;
import org.junit.jupiter.api.AfterEach;
import org.junit.jupiter.api.BeforeAll;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.testcontainers.containers.CassandraContainer;
import org.testcontainers.junit.jupiter.Container;
import org.testcontainers.junit.jupiter.Testcontainers;

/**
 * Integration test demonstrating smart batching and shared DAL functionality.
 *
 * <p>This test proves: 1. Partition-level batching with mixed years 2. Scatter-gather query across
 * year buckets 3. Shared DAL (UdtMapper, PartitionCalculator, TenantConfigRegistry) 4. No data loss
 * in batch processing
 */
@Testcontainers
class GenericPlatformIntegrationTest {

  private static final Logger log = LoggerFactory.getLogger(GenericPlatformIntegrationTest.class);

  @Container
  static CassandraContainer<?> cassandra =
      new CassandraContainer<>("cassandra:4.1").withExposedPorts(9042);

  private static CqlSession session;
  private static DynamicIngestService ingestService;
  private static DynamicRetrievalService retrievalService;
  private static TenantConfigRegistry registry;

  @BeforeAll
  static void setup() {
    // Wait for Cassandra to be ready
    cassandra.start();

    // Create CQL Session
    session =
        new CqlSessionBuilder()
            .addContactPoint(
                new InetSocketAddress(cassandra.getHost(), cassandra.getMappedPort(9042)))
            .withLocalDatacenter("datacenter1")
            .build();

    log.info("Connected to Cassandra at {}:{}", cassandra.getHost(), cassandra.getMappedPort(9042));

    // Initialize schema
    initializeSchema();

    // Initialize registry and services
    registry = new TenantConfigRegistry();
    ingestService = new DynamicIngestService(session, registry);
    retrievalService = new DynamicRetrievalService(session, registry);

    // Register tenant configuration
    registerTenant();
  }

  @AfterEach
  void cleanup() {
    // Truncate table between tests for proper test isolation
    session.execute("TRUNCATE " + SchemaInit.getKeyspace() + "." + SchemaInit.getTableName());
    log.info("Cleaned up test data");
  }

  /** Executes the schema initialization CQL. */
  private static void initializeSchema() {
    log.info("Initializing schema...");

    // Create keyspace
    session.execute(SchemaInit.getKeyspaceCreation());

    // Use the keyspace
    session.execute("USE " + SchemaInit.getKeyspace());

    // Create UDT and table
    String schema = SchemaInit.getSchema();
    String[] statements = schema.split(";");

    for (String statement : statements) {
      String trimmed = statement.trim();
      if (!trimmed.isEmpty()) {
        session.execute(trimmed);
        log.info("Executed: {}", trimmed.substring(0, Math.min(50, trimmed.length())));
      }
    }

    log.info("Schema initialized successfully");
  }

  /** Registers the tenant configuration for IBM. */
  private static void registerTenant() {
    TenantConfig config =
        TenantConfig.withBucket(
            SchemaInit.getKeyspace(),
            SchemaInit.getTableName(),
            List.of("tenant_id", "instrument_id", "period_year"),
            "period_year",
            Set.of("data") // 'data' column is a UDT
            );

    registry.register("IBM", "DAILY", "NUMERIC", config);

    log.info("Registered tenant: IBM");
  }

  /** THE PROOF: Smart batching with mixed years and scatter-gather query. */
  @Test
  void testSmartBatchingAndRouting() {
    log.info("=== Starting Smart Batching Integration Test ===");

    // Create batch with mixed years
    List<Map<String, Object>> batch = new ArrayList<>();

    // 5 rows for Dec 2023
    for (int i = 0; i < 5; i++) {
      batch.add(
          createRow("IBM", "IBM_STOCK", LocalDate.of(2023, 12, 10 + i), "revenue", 100.0 + i));
    }

    // 5 rows for Jan 2024
    for (int i = 0; i < 5; i++) {
      batch.add(createRow("IBM", "IBM_STOCK", LocalDate.of(2024, 1, 10 + i), "profit", 50.0 + i));
    }

    log.info("Step 1: Created batch with 10 rows (5 from 2023, 5 from 2024)");

    // Ingest batch
    IngestBatchRequest request = new IngestBatchRequest("IBM", "DAILY", batch);
    ingestService.ingestBatchAsync(request).join();

    log.info("Step 2: Batch ingestion complete");

    // Query across year boundary
    Map<String, Object> criteria =
        Map.of(
            "tenant_id", "IBM",
            "instrument_id", "IBM_STOCK",
            "start_date", "2023-12-01",
            "end_date", "2024-02-01");

    List<Map<String, Object>> results = retrievalService.retrieve("IBM", criteria);

    log.info("Step 3: Query returned {} rows", results.size());

    // Verify all 10 rows returned
    assertThat(results).hasSize(10);

    // Verify partition grouping worked (2 batches: 2023 and 2024)
    long count2023 =
        results.stream().filter(r -> ((LocalDate) r.get("period_date")).getYear() == 2023).count();

    long count2024 =
        results.stream().filter(r -> ((LocalDate) r.get("period_date")).getYear() == 2024).count();

    assertThat(count2023).isEqualTo(5);
    assertThat(count2024).isEqualTo(5);

    // Verify UDT conversion (data field should be a Map, not UdtValue)
    Map<String, Object> firstResult = results.get(0);
    assertThat(firstResult.get("data")).isInstanceOf(Map.class);

    @SuppressWarnings("unchecked")
    Map<String, Object> dataMap = (Map<String, Object>) firstResult.get("data");
    assertThat(dataMap).containsKeys("value", "report_time");
    assertThat(dataMap.get("value")).isInstanceOf(BigDecimal.class);
    assertThat(dataMap.get("report_time")).isInstanceOf(Instant.class);

    log.info("=== Integration Test PASSED ===");
    log.info("Successfully demonstrated:");
    log.info("  ‚úì Smart batching with partition-level grouping");
    log.info("  ‚úì Mixed year batches (2023 & 2024)");
    log.info("  ‚úì Scatter-gather query across year buckets");
    log.info("  ‚úì Shared DAL (UdtMapper, PartitionCalculator, TenantConfigRegistry)");
    log.info("  ‚úì No data loss (10/10 rows retrieved)");

    // Print sample result
    log.info("Sample result: {}", results.get(0));
  }

  /** Original test for backward compatibility. */
  @Test
  void testScatterGatherAcrossYearBuckets() {
    log.info("=== Starting Original Integration Test ===");

    // Step 1: Ingest data for IBM in December 2023
    log.info("Step 1: Ingesting data for Dec 2023...");
    ingestDataPoint("IBM", "IBM_STOCK", LocalDate.of(2023, 12, 15), "revenue", 95.5);
    ingestDataPoint("IBM", "IBM_STOCK", LocalDate.of(2023, 12, 20), "profit", 12.3);

    // Step 2: Ingest data for IBM in January 2024
    log.info("Step 2: Ingesting data for Jan 2024...");
    ingestDataPoint("IBM", "IBM_STOCK", LocalDate.of(2024, 1, 10), "revenue", 102.7);
    ingestDataPoint("IBM", "IBM_STOCK", LocalDate.of(2024, 1, 25), "profit", 15.8);

    // Step 3: Query data from 2023-12-01 to 2024-02-01 (crosses year boundary)
    log.info("Step 3: Querying data from 2023-12-01 to 2024-02-01...");

    Map<String, Object> criteria = new HashMap<>();
    criteria.put("tenant_id", "IBM");
    criteria.put("instrument_id", "IBM_STOCK");
    criteria.put("start_date", "2023-12-01");
    criteria.put("end_date", "2024-02-01");

    List<Map<String, Object>> results = retrievalService.retrieve("IBM", criteria);

    // Step 4: Verify results
    log.info("Step 4: Verifying results...");
    log.info("Retrieved {} records", results.size());

    assertThat(results).hasSize(4);

    // Verify we have data from both years
    long count2023 =
        results.stream().filter(r -> ((LocalDate) r.get("period_date")).getYear() == 2023).count();

    long count2024 =
        results.stream().filter(r -> ((LocalDate) r.get("period_date")).getYear() == 2024).count();

    assertThat(count2023).isEqualTo(2);
    assertThat(count2024).isEqualTo(2);

    log.info("=== Original Integration Test PASSED ===");
  }

  /** Helper method to create a data row. */
  private Map<String, Object> createRow(
      String tenantId, String instrumentId, LocalDate periodDate, String fieldId, double value) {
    Map<String, Object> row = new HashMap<>();
    row.put("tenant_id", tenantId);
    row.put("instrument_id", instrumentId);
    row.put("period_date", periodDate);
    row.put("field_id", fieldId);

    // Create UDT as a Map (will be converted to UdtValue by the service)
    Map<String, Object> dataPoint = new HashMap<>();
    dataPoint.put("value", BigDecimal.valueOf(value));
    dataPoint.put("report_time", Instant.now());
    row.put("data", dataPoint);

    return row;
  }

  /** Helper method to ingest a data point. */
  private void ingestDataPoint(
      String tenantId, String instrumentId, LocalDate periodDate, String fieldId, double value) {
    Map<String, Object> payload = createRow(tenantId, instrumentId, periodDate, fieldId, value);
    ingestService.ingest(tenantId, payload);
    log.info("Ingested: {} {} on {} = {}", tenantId, fieldId, periodDate, value);
  }
}
</file>

<file path="pom.xml">
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 
         http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.platform</groupId>
    <artifactId>data-platform-core</artifactId>
    <version>1.0.0-SNAPSHOT</version>
    <packaging>pom</packaging>

    <name>Data Platform Core</name>
    <description>Multi-tenant Data Platform with Dynamic Schema Support</description>

    <modules>
        <module>data-common</module>
        <module>data-ingest-service</module>
        <module>data-query-service</module>
    </modules>

    <properties>
        <java.version>21</java.version>
        <maven.compiler.source>21</maven.compiler.source>
        <maven.compiler.target>21</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        
        <!-- Spring Boot -->
        <spring-boot.version>3.2.1</spring-boot.version>
        
        <!-- Datastax Driver -->
        <cassandra-driver.version>4.17.0</cassandra-driver.version>
        
        <!-- Testing -->
        <junit.version>5.10.1</junit.version>
        <testcontainers.version>1.19.3</testcontainers.version>
        <assertj.version>3.25.1</assertj.version>
        
        <!-- Utilities -->
        <jackson.version>2.16.1</jackson.version>
        <slf4j.version>2.0.9</slf4j.version>
    </properties>

    <dependencyManagement>
        <dependencies>
            <!-- Spring Boot BOM -->
            <dependency>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-dependencies</artifactId>
                <version>${spring-boot.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>

            <!-- Datastax Cassandra Driver -->
            <dependency>
                <groupId>com.datastax.oss</groupId>
                <artifactId>java-driver-core</artifactId>
                <version>${cassandra-driver.version}</version>
            </dependency>
            <dependency>
                <groupId>com.datastax.oss</groupId>
                <artifactId>java-driver-query-builder</artifactId>
                <version>${cassandra-driver.version}</version>
            </dependency>

            <!-- Jackson for JSON -->
            <dependency>
                <groupId>com.fasterxml.jackson.core</groupId>
                <artifactId>jackson-databind</artifactId>
                <version>${jackson.version}</version>
            </dependency>

            <!-- Testing -->
            <dependency>
                <groupId>org.junit.jupiter</groupId>
                <artifactId>junit-jupiter</artifactId>
                <version>${junit.version}</version>
                <scope>test</scope>
            </dependency>
            <dependency>
                <groupId>org.testcontainers</groupId>
                <artifactId>testcontainers</artifactId>
                <version>${testcontainers.version}</version>
                <scope>test</scope>
            </dependency>
            <dependency>
                <groupId>org.testcontainers</groupId>
                <artifactId>cassandra</artifactId>
                <version>${testcontainers.version}</version>
                <scope>test</scope>
            </dependency>
            <dependency>
                <groupId>org.testcontainers</groupId>
                <artifactId>junit-jupiter</artifactId>
                <version>${testcontainers.version}</version>
                <scope>test</scope>
            </dependency>
            <dependency>
                <groupId>org.assertj</groupId>
                <artifactId>assertj-core</artifactId>
                <version>${assertj.version}</version>
                <scope>test</scope>
            </dependency>

            <!-- Internal Modules -->
            <dependency>
                <groupId>com.platform</groupId>
                <artifactId>data-common</artifactId>
                <version>${project.version}</version>
            </dependency>
        </dependencies>
    </dependencyManagement>

    <build>
        <pluginManagement>
            <plugins>
                <plugin>
                    <groupId>org.springframework.boot</groupId>
                    <artifactId>spring-boot-maven-plugin</artifactId>
                    <version>${spring-boot.version}</version>
                </plugin>
                <plugin>
                    <groupId>org.apache.maven.plugins</groupId>
                    <artifactId>maven-compiler-plugin</artifactId>
                    <version>3.12.1</version>
                    <configuration>
                        <source>${java.version}</source>
                        <target>${java.version}</target>
                    </configuration>
                </plugin>
                <plugin>
                    <groupId>org.apache.maven.plugins</groupId>
                    <artifactId>maven-surefire-plugin</artifactId>
                    <version>3.2.3</version>
                </plugin>

                <!-- Google Java Format via Spotless -->
                <plugin>
                    <groupId>com.diffplug.spotless</groupId>
                    <artifactId>spotless-maven-plugin</artifactId>
                    <version>2.43.0</version>
                    <configuration>
                        <java>
                            <!-- Explicit version required for Java 21 compatibility -->
                            <googleJavaFormat>
                                <version>1.31.0</version>
                                <style>GOOGLE</style>
                            </googleJavaFormat>
                            
                            <!-- This step also relies on javac internals, ensure it's compatible -->
                            <removeUnusedImports />
                        </java>
                    </configuration>
                    <!-- Ensure your executions block is correct -->
                    <executions>
                        <execution>
                            <goals>
                                <goal>check</goal>
                            </goals>
                            <phase>compile</phase>
                        </execution>
                    </executions>
                </plugin>

            </plugins>
        </pluginManagement>
    </build>
</project>
</file>

<file path="data-common/src/test/java/com/platform/data/common/mapper/UdtMapperTest.java">
package com.platform.data.common.mapper;

import static org.assertj.core.api.Assertions.*;

import java.util.Map;
import org.junit.jupiter.api.Test;

/**
 * Unit tests for UdtMapper.
 *
 * <p>Note: The toMap() method is primarily tested via integration tests where real UdtValue
 * instances are available. Unit testing with mocks is limited due to Mockito's inability to
 * properly mock UdtValue.getObject().
 *
 * <p>The toUdt() method requires extensive Cassandra type mocking and is better tested via
 * integration tests.
 */
class UdtMapperTest {

  @Test
  void testToMap_withNullUdtValue_returnsEmptyMap() {
    // Act
    Map<String, Object> result = UdtMapper.toMap(null);

    // Assert
    assertThat(result).isNotNull();
    assertThat(result).isEmpty();
  }

  @Test
  void testToMap_withNullField_includesNullInMap() {
    // This test verifies the null handling logic exists
    // Full testing requires integration tests with real UdtValue instances

    // The toMap() method handles null fields by including them in the map
    // This is verified in integration tests where real Cassandra UDTs are used

    // For now, just verify the method signature and null input handling
    Map<String, Object> result = UdtMapper.toMap(null);
    assertThat(result).isNotNull();
  }
}
</file>

</files>
